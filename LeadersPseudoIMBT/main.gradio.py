# -*- coding: utf-8 -*-
import gradio as gr
from modules.survey import get_questions_with_options
from modules.advice import get_advice_text
import openai
import json

# Global variable to store API configuration
current_api_config = {}

def generate_ai_analysis(questions, answers_dict, api_key, base_url, model):
    """
    Generate AI-powered analysis using OpenAI API with all questions and answers
    """
    print("ğŸ¯ generate_ai_analysis å‡½æ•°è¢«è°ƒç”¨")
    print(f"ğŸ“Š é—®é¢˜æ•°é‡: {len(questions)}")
    print(f"ğŸ“ ç­”æ¡ˆæ•°é‡: {len(answers_dict)}")

    # Use the provided API configuration instead of global variable
    api_config = {
        'api_key': api_key,
        'base_url': base_url,
        'model': model
    }

    print(f"ğŸ”§ å½“å‰APIé…ç½®: {api_config}")

    # Check if API configuration is available
    if not api_config.get('api_key') or not api_config.get('base_url') or not api_config.get('model'):
        print("âš ï¸ APIé…ç½®ä¸å®Œæ•´ï¼Œä½¿ç”¨fallbackæ¨¡å¼")
        # Fallback to basic analysis without traditional scoring
        return "âš ï¸ AIåˆ†ææœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚\n\nè¯·æ£€æŸ¥æ‚¨çš„APIé…ç½®ï¼ˆ.envæ–‡ä»¶ï¼‰ï¼Œç¡®ä¿åŒ…å«æœ‰æ•ˆçš„API_KEYã€BASE_URLå’ŒMODELè®¾ç½®ã€‚\n\næ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼é…ç½®ï¼š\n1. å¤åˆ¶ .env.example ä¸º .env\n2. å¡«å…¥æ‚¨çš„OpenAI APIå¯†é’¥\n3. è®¾ç½®æ­£ç¡®çš„APIåœ°å€å’Œæ¨¡å‹åç§°"

    try:
        print("ğŸ”— åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯...")
        # Initialize OpenAI client
        client = openai.OpenAI(
            api_key=api_config['api_key'],
            base_url=api_config['base_url']
        )
        print("âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")

        # Prepare all questions and answers
        print("ğŸ“ å‡†å¤‡é—®é¢˜å’Œç­”æ¡ˆæ•°æ®...")
        qa_pairs = []
        for i, (question, options) in enumerate(questions):
            q_id = i + 1
            if q_id in answers_dict:
                answer = answers_dict[q_id]
                qa_pairs.append(f"é—®é¢˜{q_id}: {question}\nå›ç­”: {answer}")

        qa_text = "\n\n".join(qa_pairs)
        print(f"ğŸ“‹ å‡†å¤‡äº†{len(qa_pairs)}ä¸ªé—®ç­”å¯¹")

        print("ğŸ“ æ„å»ºAIæç¤ºè¯...")
        prompt = f"""åŸºäºä»¥ä¸‹é¢†å¯¼ç‰¹æ€§è°ƒç ”çš„æ‰€æœ‰é—®é¢˜å’Œç­”æ¡ˆï¼Œè¯·ä¸ºç”¨æˆ·æä¾›ä¸“ä¸šçš„é¢†å¯¼ç±»å‹åˆ†æå’Œæ²Ÿé€šå»ºè®®ã€‚

## è°ƒç ”é—®ç­”è¯¦æƒ…ï¼š
{qa_text}

## åˆ†æè¦æ±‚ï¼š
è¯·åŸºäºä»¥ä¸Šæ‰€æœ‰é—®é¢˜å’Œç­”æ¡ˆï¼Œå…¨é¢åˆ†æè¿™ä½é¢†å¯¼çš„ç‰¹æ€§ï¼ŒæŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºåˆ†æç»“æœï¼š

### ğŸ¦Š é¢†å¯¼ç±»å‹åˆ¤æ–­
[æ ¹æ®æ‰€æœ‰å›ç­”åˆ¤æ–­é¢†å¯¼å±äºå“ªç§åŠ¨ç‰©ç±»å‹ï¼Œå¹¶ç»™å‡ºåˆ¤æ–­ä¾æ®]
- ç‹¡çŒ¾çš„ç‹ç‹¸ï¼šç²¾æ˜ã€ç­–ç•¥æ€§å¼ºã€æ³¨é‡åˆ©ç›Š
- ç‹¼ç¾¤äºŒæŠŠæ‰‹ï¼šå¼ºåŠ¿ã€ç«äº‰æ€§ã€å›¢é˜Ÿé¢†å¯¼åŠ›
- æ™ºæ…§çš„çŒ«å¤´é¹°ï¼šç†æ€§ã€åˆ†æåŠ›å¼ºã€æ³¨é‡ç»†èŠ‚
- æ¸©å’Œçš„å…”å­ï¼šæ¸©å’Œã€åŒ…å®¹æ€§ã€æ³¨é‡å’Œè°
- å‹‡çŒ›çš„ç‹®å­ï¼šè‡ªä¿¡ã€å†³ç­–åŠ›å¼ºã€é¢†å¯¼é­…åŠ›
- å‹¤åŠ³çš„èœœèœ‚ï¼šå‹¤å¥‹ã€è´£ä»»å¿ƒå¼ºã€æ³¨é‡æ•ˆç‡

### ğŸ“Š ç»¼åˆç‰¹æ€§åˆ†æ
è¯¦ç»†åˆ†æé¢†å¯¼çš„ä¸»è¦ç‰¹æ€§ï¼š
- å·¥ä½œæ€åº¦ï¼šç§¯ææ€§ã€è´£ä»»å¿ƒã€æ‰§è¡ŒåŠ›
- æ²Ÿé€šæ–¹å¼ï¼šè¡¨è¾¾é£æ ¼ã€å€¾å¬èƒ½åŠ›ã€åé¦ˆæ–¹å¼
- ç®¡ç†é£æ ¼ï¼šå†³ç­–æ–¹å¼ã€å›¢é˜Ÿç®¡ç†ã€æˆæƒç¨‹åº¦
- äººé™…å…³ç³»ï¼šåŒäº‹ç›¸å¤„ã€ä¸Šä¸‹çº§å…³ç³»ã€å†²çªå¤„ç†
- é¢†å¯¼é­…åŠ›ï¼šå½±å“åŠ›ã€æ¿€åŠ±æ–¹å¼ã€å›¢é˜Ÿå‡èšåŠ›

### ğŸ’¡ ä¸ªæ€§åŒ–æ²Ÿé€šå»ºè®®
åŸºäºä»¥ä¸Šåˆ†æï¼Œæä¾›å…·ä½“çš„æ²Ÿé€šç­–ç•¥ï¼š
1. **æ—¥å¸¸æ²Ÿé€š**ï¼šæœ€ä½³æ²Ÿé€šæ—¶æœºã€æ–¹å¼å’Œè¯é¢˜é€‰æ‹©
2. **å·¥ä½œæ±‡æŠ¥**ï¼šæ±‡æŠ¥å†…å®¹ç»„ç»‡ã€æ—¶æœºæŠŠæ¡ã€é‡ç‚¹çªå‡º
3. **æ„è§è¡¨è¾¾**ï¼šæå‡ºå»ºè®®çš„æ–¹å¼ã€æ—¶æœºé€‰æ‹©ã€è¯´æœæŠ€å·§
4. **å†²çªå¤„ç†**ï¼šé¢å¯¹åˆ†æ­§æ—¶çš„åº”å¯¹ç­–ç•¥ã€åŒ–è§£æ–¹æ³•
5. **èŒä¸šå‘å±•**ï¼šå¦‚ä½•äº‰å–æœºä¼šã€å±•ç°èƒ½åŠ›ã€å»ºç«‹å…³ç³»
6. **æ³¨æ„äº‹é¡¹**ï¼šéœ€è¦é¿å…çš„è¡Œä¸ºã€æ½œåœ¨é£é™©ã€æ”¹è¿›æ–¹å‘

è¯·ç”¨ä¸“ä¸šã€å»ºè®¾æ€§çš„è¯­è¨€è¾“å‡ºï¼Œç¡®ä¿åˆ†æå®¢è§‚å‡†ç¡®ï¼Œå»ºè®®å®ç”¨å¯è¡Œã€‚"""

        print("ğŸš€ æ­£åœ¨è°ƒç”¨OpenAI API...")
        # Call OpenAI API
        response = client.chat.completions.create(
            model=api_config['model'],
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä½èµ„æ·±ç»„ç»‡è¡Œä¸ºå­¦ä¸“å®¶å’Œé¢†å¯¼åŠ›æ•™ç»ƒï¼Œæ“…é•¿é€šè¿‡é—®å·æ•°æ®åˆ†æé¢†å¯¼ç‰¹æ€§å¹¶æä¾›ç²¾å‡†çš„æ²Ÿé€šå»ºè®®ã€‚è¯·åŸºäºå®Œæ•´çš„è°ƒç ”æ•°æ®ç»™å‡ºå…¨é¢ã€å®ç”¨çš„åˆ†æã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=16000,
            temperature=0.7
        )
        print("âœ… OpenAI APIè°ƒç”¨æˆåŠŸ")

        ai_analysis = response.choices[0].message.content.strip()
        print(f"ğŸ“„ æ”¶åˆ°çš„AIåˆ†æé•¿åº¦: {len(ai_analysis)}")
        return ai_analysis

    except Exception as e:
        print(f"AIåˆ†æå¤±è´¥: {e}")
        # Fallback to basic analysis without traditional scoring
        return f"âš ï¸ AIåˆ†ææœåŠ¡æš‚æ—¶å‡ºç°é”™è¯¯ï¼š{str(e)}\n\nè¯·æ£€æŸ¥æ‚¨çš„ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®ã€‚\n\nå¦‚æœé—®é¢˜æŒç»­å­˜åœ¨ï¼Œè¯·å°è¯•ï¼š\n1. éªŒè¯APIå¯†é’¥æ˜¯å¦æœ‰æ•ˆ\n2. æ£€æŸ¥ç½‘ç»œè¿æ¥\n3. ç¡®è®¤ä½¿ç”¨çš„æ¨¡å‹æ˜¯å¦å¯ç”¨"

def process_answers(*args):
    """
    Process user answers and generate analysis results with loading state
    """
    print("ğŸ” process_answers å‡½æ•°è¢«è°ƒç”¨")
    print(f"ğŸ“Š æ”¶åˆ°çš„å‚æ•°æ•°é‡: {len(args)}")

    # æœ€åä¸‰ä¸ªå‚æ•°æ˜¯APIé…ç½®ï¼Œå‰é¢çš„éƒ½æ˜¯ç­”æ¡ˆ
    num_answers = len(args) - 3
    answers = args[:num_answers]
    api_key, base_url, model = args[-3:]

    print(f"ğŸ“Š ç­”æ¡ˆæ•°é‡: {len(answers)}")
    print(f"ğŸ”§ APIé…ç½®: key={api_key[:10]}..., url={base_url}, model={model}")

    # Convert answers to dictionary format
    answers_dict = {}
    questions = get_questions_with_options()
    print(f"ğŸ“‹ åŠ è½½çš„é—®é¢˜æ•°é‡: {len(questions)}")

    for i, answer in enumerate(answers):
        if i < len(questions):
            answers_dict[i + 1] = answer  # Question ID starts from 1
            print(f"âœ… é—®é¢˜{i+1}: {answer}")
        else:
            print(f"âš ï¸ é¢å¤–ç­”æ¡ˆ{i}: {answer}")

    print(f"ğŸ“ ç­”æ¡ˆå­—å…¸å†…å®¹: {answers_dict}")

    # é¦–å…ˆè¿”å›åŠ è½½çŠ¶æ€å’Œè·³è½¬åˆ°ç»“æœé¡µé¢
    loading_message = """ğŸ¤– AIåˆ†æè¿›è¡Œä¸­...

â³ æ­£åœ¨åˆ†ææ‚¨çš„å›ç­”...
â³ æ­£åœ¨ç”Ÿæˆé¢†å¯¼ç±»å‹åˆ¤æ–­...
â³ æ­£åœ¨å‡†å¤‡ä¸ªæ€§åŒ–æ²Ÿé€šå»ºè®®...

è¯·ç¨å€™ï¼Œåˆ†æéœ€è¦10-30ç§’...

ğŸ’¡ æç¤ºï¼šåˆ†æå®Œæˆåå°†è‡ªåŠ¨æ˜¾ç¤ºå®Œæ•´æŠ¥å‘Š"""
    yield loading_message, gr.update(selected=2)

    # Generate AI-powered analysis with all questions and answers
    print("ğŸ¤– å¼€å§‹ç”ŸæˆAIåˆ†æ...")
    print("ğŸ“Š æ­£åœ¨å‡†å¤‡æ•°æ®...")
    analysis_result = generate_ai_analysis(questions, answers_dict, api_key, base_url, model)
    print(f"ğŸ“„ AIåˆ†æç»“æœé•¿åº¦: {len(analysis_result)}")
    print(f"ğŸ“„ AIåˆ†æç»“æœé¢„è§ˆ: {analysis_result[:200]}...")

    print("âœ… process_answers å‡½æ•°æ‰§è¡Œå®Œæˆ")
    yield analysis_result, gr.update(selected=2)

def validate_and_start(api_key, base_url, model):
    """
    Validate API configuration and start survey (optional)
    """
    # Store configuration for later use (even if empty)
    global current_api_config
    current_api_config = {
        'api_key': api_key,
        'base_url': base_url,
        'model': model
    }

    # Always proceed to survey tab
    return gr.update(selected=1)

def load_api_config():
    """
    Load API configuration from .env file
    """
    try:
        import os
        from dotenv import load_dotenv

        # Load .env file
        load_dotenv()

        api_key = os.getenv('API_KEY', '')
        base_url = os.getenv('BASE_URL', '')
        model = os.getenv('MODEL', '')

        return api_key, base_url, model
    except ImportError:
        # If python-dotenv is not installed, try to read .env file manually
        try:
            import os
            env_file = os.path.join(os.path.dirname(__file__), '.env')

            if os.path.exists(env_file):
                config = {}
                with open(env_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            key, value = line.split('=', 1)
                            config[key.strip()] = value.strip()

                return (
                    config.get('API_KEY', ''),
                    config.get('BASE_URL', ''),
                    config.get('MODEL', '')
                )
            else:
                return '', '', ''
        except Exception as e:
            return '', '', ''
    except Exception as e:
        return '', '', ''

def create_interface():
    """
    Create Gradio interface with tabs
    """
    questions = get_questions_with_options()

    # Load saved API configuration
    saved_api_key, saved_base_url, saved_model = load_api_config()

    with gr.Blocks(title="é¢†å¯¼ç‰¹æ€§è°ƒç ”å·¥å…·", theme=gr.themes.Soft()) as interface:
        with gr.Tabs() as tabs:
            with gr.TabItem("é¡¹ç›®ä»‹ç»", id=0):
                gr.Markdown("# é¢†å¯¼ç‰¹æ€§è°ƒç ”ä¸åˆ†æå·¥å…·")
                gr.Markdown("""
                ### é¡¹ç›®ç®€ä»‹
                æœ¬å·¥å…·åŸºäºå¤§æ¨¡å‹æŠ€æœ¯å¯¹é¢†å¯¼è¿›è¡Œæ™ºèƒ½è¯„ä»·ï¼Œé€šè¿‡ç§‘å­¦çš„é—®å·è°ƒæŸ¥åˆ†æé¢†å¯¼çš„æ€§æ ¼ç‰¹å¾ã€å†³ç­–é£æ ¼å’Œç®¡ç†æ–¹å¼ã€‚

                ### æ ¸å¿ƒåŠŸèƒ½
                - **é¢†å¯¼ç±»å‹è¯†åˆ«**ï¼šåŸºäºæ‚¨çš„å›ç­”ï¼Œç³»ç»Ÿä¼šä¸ºæ‚¨åŒ¹é…æœ€ç›¸ä¼¼çš„é¢†å¯¼ç±»å‹ï¼ˆå¦‚ç‹¡çŒ¾çš„ç‹ç‹¸ã€ç‹¼ç¾¤äºŒæŠŠæ‰‹ã€æ™ºæ…§çš„çŒ«å¤´é¹°ç­‰ï¼‰
                - **æ²Ÿé€šå»ºè®®ç”Ÿæˆ**ï¼šæ ¹æ®é¢†å¯¼ç±»å‹ï¼Œæä¾›ä¸ªæ€§åŒ–çš„äº¤äº’ç­–ç•¥å’Œæ²Ÿé€šæŠ€å·§
                - **ç‰¹æ€§åˆ†ææŠ¥å‘Š**ï¼šç”Ÿæˆè¯¦ç»†çš„é¢†å¯¼ç‰¹æ€§åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬ä¼˜åŠ¿ã€æ½œåœ¨é£é™©å’Œæ”¹è¿›å»ºè®®

                ### ä½¿ç”¨è¯´æ˜
                1. ç‚¹å‡»"å¼€å§‹æµ‹è¯„"è¿›å…¥ç­”é¢˜ç•Œé¢
                2. è®¤çœŸå›ç­”30ä¸ªé—®é¢˜ï¼ˆçº¦5-10åˆ†é’Ÿï¼‰
                3. æŸ¥çœ‹AIç”Ÿæˆçš„é¢†å¯¼ç±»å‹åˆ¤æ–­å’Œæ²Ÿé€šå»ºè®®

                ### æ³¨æ„äº‹é¡¹
                - è¯·æ ¹æ®å®é™…è§‚å¯Ÿå’Œç»å†é€‰æ‹©æœ€ç¬¦åˆçš„é€‰é¡¹
                - ç³»ç»Ÿä¼šåŸºäºå¤§æ¨¡å‹ç®—æ³•è¿›è¡Œæ™ºèƒ½åˆ†æ
                - ç»“æœä»…ä¾›å‚è€ƒï¼Œå¸®åŠ©æ‚¨æ›´å¥½åœ°ç†è§£å’Œæ²Ÿé€š
                """)

                gr.Markdown("### AIæ¨¡å‹é…ç½®")
                with gr.Row():
                    api_key_input = gr.Textbox(
                        label="API Key",
                        placeholder="è¾“å…¥æ‚¨çš„APIå¯†é’¥",
                        type="password",
                        value=saved_api_key
                    )
                    base_url_input = gr.Textbox(
                        label="Base URL",
                        placeholder="https://api.openai.com/v1",
                        value=saved_base_url
                    )
                    model_input = gr.Textbox(
                        label="Model",
                        placeholder="gpt-3.5-turbo",
                        value=saved_model
                    )

                start_btn = gr.Button("å¼€å§‹æµ‹è¯„", variant="primary", size="lg")

            with gr.TabItem("ç­”é¢˜ç•Œé¢", id=1):
                gr.Markdown("## è¯·å›ç­”ä¸‹åˆ—é—®é¢˜")
                gr.Markdown("è¯·æ ¹æ®æ‚¨çš„å®é™…æƒ…å†µé€‰æ‹©æœ€ç¬¦åˆçš„é€‰é¡¹ã€‚")

                inputs = []
                # æ¨ªå¼æ’ç‰ˆï¼šæ¯è¡Œ4ä¸ªé—®é¢˜ï¼Œä»å·¦åˆ°å³ï¼Œä»ä¸Šåˆ°ä¸‹
                for i in range(0, len(questions), 4):
                    with gr.Row():
                        for j in range(4):
                            if i + j < len(questions):
                                question, options = questions[i + j]
                                radio = gr.Radio(
                                    label=f"{i+j+1}. {question}",
                                    choices=options,
                                    value=options[0]  # è®¾ç½®é»˜è®¤å€¼ä¸ºç¬¬ä¸€ä¸ªé€‰é¡¹
                                )
                                inputs.append(radio)

                submit_btn = gr.Button("æäº¤åˆ†æ", variant="primary")

            with gr.TabItem("ç»“æœç•Œé¢", id=2):
                gr.Markdown("## åˆ†æç»“æœ")

                analysis_output = gr.Textbox(label="AIåˆ†ææŠ¥å‘Š", lines=25)

        # Button actions
        start_btn.click(
            fn=lambda api_key, base_url, model: validate_and_start(api_key, base_url, model),
            inputs=[api_key_input, base_url_input, model_input],
            outputs=[tabs]
        )

        submit_btn.click(
            fn=process_answers,
            inputs=inputs + [api_key_input, base_url_input, model_input],
            outputs=[analysis_output, tabs],
            show_progress=True
        )

        # Clean up old chart files on startup
        import os
        chart_dir = os.path.join(os.path.dirname(__file__), 'charts')
        if os.path.exists(chart_dir):
            for file in os.listdir(chart_dir):
                if file.endswith('.png'):
                    try:
                        os.remove(os.path.join(chart_dir, file))
                    except:
                        pass

    return interface

if __name__ == "__main__":
    print("Starting Leader Characteristics Survey Tool...")
    interface = create_interface()
    print("Interface created successfully. Launching server...")
    interface.launch(server_name="0.0.0.0", server_port=7861, show_error=True, share=False)
