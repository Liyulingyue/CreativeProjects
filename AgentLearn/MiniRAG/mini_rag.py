#!/usr/bin/env python
"""my_rag.py - æç®€ RAG Agent | Retrieval-Augmented Generation with file search"""
from openai import OpenAI
from dotenv import load_dotenv
import os, json, sys

from pathlib import Path
from search_utils import search_files, search_filenames, semantic_search
from tools import TOOLS

# å°è¯•åŠ è½½å½“å‰ç›®å½•çš„.env
load_dotenv()
# å¦‚æœæ²¡æœ‰MODEL_KEYï¼Œå°è¯•ä¸Šä¸€çº§ç›®å½•çš„.env
if not os.getenv("MODEL_KEY"):
    parent_env = Path("..") / ".env"
    if parent_env.exists():
        load_dotenv(dotenv_path=parent_env)

api_key = os.getenv("MODEL_KEY")
base_url = os.getenv("MODEL_URL")
model_name = os.getenv("MODEL_NAME", "gpt-4")

client = OpenAI(api_key=api_key, base_url=base_url)
SYSTEM = f"""You are an intelligent RAG agent at {os.getcwd()}. Your role is to answer questions using retrieval-augmented generation.

## æ ¸å¿ƒæ£€ç´¢ç­–ç•¥ (Core Retrieval Strategy):
1. **åŒè½¨å¹¶è¡Œ**: 
   - **å‘é‡åŒ–æ£€ç´¢ (semantic_search)**: æ“…é•¿æ•æ‰â€œå«ä¹‰â€ï¼Œè·¨è¶Šè¯ä¹‰éšœç¢ã€‚ç”¨äºå¿«é€Ÿå®šä½ç›¸å…³æ¦‚å¿µå’ŒåŸç†ã€‚
   - **å…³é”®å­—æ£€ç´¢ (search_files)**: æ“…é•¿â€œç²¾å‡†æ‰“å‡»â€å’Œâ€œå®Œæ•´è·å–â€ã€‚ç”¨äºæŸ¥æ‰¾ç‰¹å®šä»£ç ã€ç§æœ‰åè¯ã€æŠ¥é”™ä¿¡æ¯æˆ–éœ€è¦è·å–åŒ…å«æŸè¯çš„æ‰€æœ‰ç²¾ç¡®ä¸Šä¸‹æ–‡ã€‚
2. **äº’è¡¥ä¸äº¤å‰éªŒè¯**: 
   - åŒä¸€ä¸ªäº‹æƒ…å¯èƒ½æœ‰å¤šç§æè¿°æ–¹å¼ï¼Œå¦‚æœä¸€ç§æœç´¢æ–¹å¼ç»“æœä¸ä½³ï¼Œ**å¿…é¡»**åˆ‡æ¢å¦ä¸€ç§æˆ–å°è¯•åŒä¹‰è¯ã€‚
   - å¯¹äºå¤æ‚é—®é¢˜ï¼Œåº”åŒæ—¶ç»“åˆä¸¤ç§æ–¹å¼ï¼šç”¨å‘é‡æœç´¢ç¡®å®šâ€œåœ¨å“ªèŠè¿‡â€ï¼Œç”¨å…³é”®å­—æœç´¢ç¡®ä¿â€œæ²¡æ¼æ‰ç»†èŠ‚â€ã€‚

## æ€è€ƒé€»è¾‘ (Thinking Process):
1. **æ„å›¾æ‹†è§£**: å°†é—®é¢˜æ‹†è§£ä¸ºâ€œè¯­ä¹‰æ¦‚å¿µâ€å’Œâ€œç‰¹å¾å…³é”®è¯â€ã€‚
2. **çµæ´»æ£€ç´¢è·¯å¾„**:
   - **è·¯å¾„ A (æ‰«æç»“æ„)**: å¦‚æœæ–‡ä»¶åå¯èƒ½åŒ…å«å…³é”®è¯ï¼Œå…ˆç”¨ `search_filenames` å®šä½ç›¸å…³æ–‡ä»¶ã€‚
   - **è·¯å¾„ B (å†…å®¹æº¯æº/åæ¨)**: å¦‚æœæ–‡ä»¶åä¸ç›´è§‚æˆ–æœç´¢æ— æœï¼Œç›´æ¥è¿›è¡Œå…¨å±€ `semantic_search` æˆ– `search_files`ã€‚**ä»ç»“æœä¸­è§‚å¯Ÿ `file_path` å­—æ®µ**ï¼Œé€šè¿‡å†…å®¹å‘½ä¸­åæ¨å¹¶é”å®šç›®æ ‡æ–‡ä»¶ï¼Œåç»­å†åˆ©ç”¨è·¯å¾„å‚æ•°è¿›è¡Œå­åŒºåŸŸæœç´¢ã€‚
3. **å¤šé˜¶æ®µæ¢ç´¢**:
   - **ç¬¬ä¸€é˜¶æ®µ (å¹¿åº¦æ¢ç´¢)**: å…¨å±€æœç´¢é”å®šç›¸å…³æ–‡ä»¶è·¯å¾„ã€‚
   - **ç¬¬äºŒé˜¶æ®µ (é’ˆå¯¹æ€§æ·±å…¥)**: ä½¿ç”¨é”å®šæ–‡ä»¶çš„ `file_path` å‚æ•°é™åˆ¶èŒƒå›´ï¼Œè¿›è¡Œé«˜é¢‘æ¬¡çš„å…³é”®å­—/å‘é‡æœç´¢ä»¥è·å–å®Œæ•´ã€å‡†ç¡®ä¿¡æ¯ã€‚
4. **è‡ªæ„ˆä¸é‡è¯•**: å¦‚æœç»“æœä¸º "(no matches found)"ï¼Œè‡ªåŠ¨å°è¯•åŒä¹‰è¯ã€æ‰©å¤§æœç´¢èŒƒå›´æˆ–æš‚æ—¶æ”¾å¼ƒè·¯å¾„é™åˆ¶ã€‚

## å·¥å…·ä½¿ç”¨è§„èŒƒ:
- **search_filenames**: äº†è§£ç›®å½•ã€å®šä½æ–‡ä»¶ã€‚å¦‚æœåç§°æ— æ³•åˆ¤æ–­ï¼Œè¯·æœæ–­é€šè¿‡å†…å®¹æœç´¢æ¥â€œåæ¨â€æ–‡ä»¶ä½ç½®ã€‚
- **semantic_search**: æœåŸç†ã€æœåœºæ™¯ã€æœæ¨¡ç³Šæè¿°ã€‚æ”¯æŒ `file_path` ç¼©å°èŒƒå›´ã€‚
- **search_files**: æœä»£ç ã€æœé…ç½®ã€æœæœ¯è¯­ã€‚æ”¯æŒ `file_path` ç¼©å°èŒƒå›´ã€‚
- **æ³¨æ„**: æœç´¢ç»“æœä¸­çš„ `file_path`ï¼ˆå¦‚ `knowledge/coffee.txt`ï¼‰æ˜¯æä½³çš„å®šä½çº¿ç´¢ï¼Œè¯·å­¦ä¼šæ ¹æ®å†…å®¹åæ¨å‡ºæ–‡ä»¶åã€‚

## è§„åˆ™:
- åœ¨æ‰§è¡Œæœç´¢å‰ï¼Œç®€è¦è¯´æ˜ä½ çš„æ€è€ƒè¿‡ç¨‹å’Œæœç´¢ç­–ç•¥ã€‚
- ç¦æ­¢åœ¨æ²¡æœ‰å°è¯•æœç´¢çš„æƒ…å†µä¸‹ç›´æ¥å›ç­”â€œä¸çŸ¥é“â€ã€‚
- æ¯æ¬¡ä»»åŠ¡æœ€å¤šå°è¯• 3 æ¬¡ä¸åŒçš„æœç´¢ç»„åˆã€‚
"""

def chat(prompt, history=[], stream=False):
    """RAGå¯¹è¯å‡½æ•°
    
    Args:
        prompt: ç”¨æˆ·è¾“å…¥
        history: å¯¹è¯å†å²
        stream: æ˜¯å¦è¿”å›ç”Ÿæˆå™¨è¿›è¡Œæµå¼è¾“å‡º
    
    Returns:
        å¦‚æœstream=Trueï¼Œè¿”å›ç”Ÿæˆå™¨ï¼›å¦åˆ™è¿”å›æœ€ç»ˆç»“æœå­—ç¬¦ä¸²
    """
    if stream:
        return chat_stream(prompt, history)
    else:
        # éæµå¼æ¨¡å¼ï¼Œæ”¶é›†æ‰€æœ‰è¾“å‡ºåè¿”å›
        result = ""
        for chunk in chat_stream(prompt, history):
            if chunk.get("type") == "final":
                result = chunk.get("content", "")
        return result

def chat_stream(prompt, history=[]):
    """RAGå¯¹è¯çš„æµå¼ç”Ÿæˆå™¨"""
    history.append({"role": "user", "content": prompt})
    
    iteration = 0
    while True:
        iteration += 1
        messages = [{"role": "system", "content": SYSTEM}] + history
        
        # Yieldæ€è€ƒçŠ¶æ€
        yield {"type": "thinking", "content": f"ğŸ¤” æ­£åœ¨æ€è€ƒ... (è¿­ä»£ {iteration})"}
        
        r = client.chat.completions.create(model=model_name, messages=messages, tools=TOOLS, max_tokens=8000)
        message = r.choices[0].message
        content = message.content
        tool_calls = message.tool_calls
        
        if content:
            print(f"\033[32mAgent: {content}\033[0m")
            # Yield Agentçš„æ€è€ƒå†…å®¹
            yield {"type": "agent_thought", "content": content}

        if tool_calls:
            history.append({"role": "assistant", "content": content, "tool_calls": tool_calls})
            for tool_call in tool_calls:
                fn_name = tool_call.function.name
                args = json.loads(tool_call.function.arguments)
                
                # Yieldå·¥å…·è°ƒç”¨ä¿¡æ¯
                tool_info = f"ğŸ”§ è°ƒç”¨å·¥å…·: {fn_name}\nå‚æ•°: {json.dumps(args, ensure_ascii=False, indent=2)}"
                print(f"\033[33mCalling tool: {fn_name}({args})\033[0m")
                yield {"type": "tool_call", "content": tool_info, "tool_name": fn_name, "args": args}
                
                if fn_name == "search_files":
                    output = search_files(args.get('keyword'), args.get('file_path'))
                elif fn_name == "search_filenames":
                    output = search_filenames(args.get('keyword'))
                elif fn_name == "semantic_search":
                    output = semantic_search(args.get('query'), file_path=args.get('file_path'))
                else:
                    output = f"Error: Unknown tool {fn_name}"
                
                # Yieldå·¥å…·æ‰§è¡Œç»“æœ
                result_preview = output[:500] + "..." if len(output) > 500 else output
                result_info = f"ğŸ“Š å·¥å…·ç»“æœ:\n{result_preview}"
                print(f"\033[34mResult: {output[:500]}...\033[0m")
                yield {"type": "tool_result", "content": result_info, "full_result": output}
                
                history.append({"role": "tool", "tool_call_id": tool_call.id, "content": output[:50000]})
        else:
            history.append({"role": "assistant", "content": content})
            # Yieldæœ€ç»ˆç­”æ¡ˆ
            yield {"type": "final", "content": content}
            return

if __name__ == "__main__":
    if len(sys.argv) > 1: 
        # å­ä»£ç†æ¨¡å¼ - æµå¼è¾“å‡º
        for chunk in chat_stream(sys.argv[1]):
            if chunk.get("type") == "agent_thought":
                print(f"\nğŸ’­ æ€è€ƒ: {chunk['content']}")
            elif chunk.get("type") == "tool_call":
                print(f"\n{chunk['content']}")
            elif chunk.get("type") == "tool_result":
                print(f"\n{chunk['content']}")
            elif chunk.get("type") == "final":
                print(f"\nâœ… æœ€ç»ˆç­”æ¡ˆ:\n{chunk['content']}")
    else:
        # äº¤äº’æ¨¡å¼ - æµå¼è¾“å‡º
        h = []
        while (q := input("\033[36m>> \033[0m")) not in ("q", "exit", "bye", ""): 
            print("\n" + "="*60)
            for chunk in chat_stream(q, h):
                if chunk.get("type") == "thinking":
                    print(f"\n{chunk['content']}", end="\n", flush=True)
                elif chunk.get("type") == "agent_thought":
                    print(f"\nğŸ’­ æ€è€ƒ: {chunk['content']}", flush=True)
                elif chunk.get("type") == "tool_call":
                    print(f"\n{chunk['content']}", flush=True)
                elif chunk.get("type") == "tool_result":
                    print(f"\n{chunk['content']}", flush=True)
                elif chunk.get("type") == "final":
                    print(f"\nâœ… æœ€ç»ˆç­”æ¡ˆ:\n{chunk['content']}", flush=True)
            print("\n" + "="*60)