# PaddleOCR-VL GGUF 版本 - 简明教程

## 💡 核心思想

将 PaddleOCR-VL 拆分为两部分:
- **视觉编码器** (PyTorch) - 处理图像
- **语言模型** (llama.cpp/GGUF) - 生成文字

**收益**: 内存减少 70%, 速度提升 2-3 倍!

## 🚀 三步快速开始

### 步骤 1: 安装依赖

```bash
# 基础依赖
pip install torch transformers fastapi uvicorn pillow einops

# llama.cpp Python 绑定 (核心!)
pip install llama-cpp-python

# GPU 加速 (可选)
# CUDA: CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python
# Mac:  CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python
```

### 步骤 2: 提取和转换模型

```bash
# 2.1 提取 LLM 权重
python convert_to_gguf.py

# 2.2 转换为 GGUF (需要 llama.cpp)
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp && make

# 转换权重 (需要适配脚本)
python convert.py ../extracted_llm --outfile ../extracted_llm/llm_model.gguf

# 2.3 量化 (推荐 Q4_K_M, 节省内存)
./quantize ../extracted_llm/llm_model.gguf \
           ../extracted_llm/llm_model_q4.gguf Q4_K_M
```

### 步骤 3: 启动和测试

```bash
# 启动服务器
python demo_ppocrvl_gguf_server.py

# 测试 (另一个终端)
python demo_ppocrvl_gguf_client.py \
    --text "识别文字" \
    --image test.jpg
```

## 📊 性能对比

| 模式 | 内存 | 速度 | 质量 |
|------|------|------|------|
| 原始 PyTorch | 4GB | 1x | 100% |
| GGUF Q4_K_M | 1.2GB ✨ | 2-3x ⚡ | ~98% |

## 🔧 配置调优

编辑 `demo_ppocrvl_gguf_server.py`:

```python
GGUF_MODEL_PATH = "extracted_llm/llm_model_q4.gguf"  # 模型路径
N_GPU_LAYERS = 0      # GPU 层数 (0=纯CPU, 32=全GPU)
N_CTX = 4096          # 上下文长度
N_THREADS = 8         # CPU 线程数
```

**提示**:
- CPU 用户: `N_GPU_LAYERS=0`, `N_THREADS=你的CPU核心数`
- GPU 用户: `N_GPU_LAYERS=32` (或更高)

## 📦 量化级别选择

| 量化 | 大小 | 速度 | 质量 | 推荐 |
|------|------|------|------|------|
| Q4_0 | 最小 | 最快 | 较低 | 快速测试 |
| **Q4_K_M** | 小 | 快 | 好 | ✅ 推荐 |
| Q5_K_M | 中 | 中 | 很好 | 高质量 |
| Q8_0 | 大 | 慢 | 最好 | 接近原始 |

**建议**: 先用 Q4_K_M, 如果质量不够再试 Q5_K_M

## ❓ 常见问题

### Q1: 为什么不用 Ollama?
**A**: llama.cpp 更直接,更灵活,减少中间层开销。

### Q2: GGUF 转换很复杂?
**A**: 是的,因为 Ernie4.5 不是标准 Llama 架构。我们提供了详细指南,但可能需要自定义转换脚本。

### Q3: 需要 GPU 吗?
**A**: 不需要! GGUF 在 CPU 上也很快。有 GPU 更好,但不是必须的。

### Q4: 精度损失大吗?
**A**: Q4_K_M 通常只损失 2-3%,实际使用中几乎察觉不到。

### Q5: 如果转换失败怎么办?
**A**: 可以:
- 使用原始 PyTorch 版本 (在 PaddleOCR-VL-CPU 目录)
- 等待社区贡献转换脚本
- 尝试其他后端 (vLLM, TensorRT-LLM)

## 🎯 适用场景

✅ **适合使用 GGUF**:
- 内存有限 (< 8GB)
- 需要 CPU 推理
- 需要低延迟
- 批量文档处理

❌ **不适合 GGUF**:
- 需要最高精度
- 已有强大 GPU
- 只偶尔使用

## 📁 项目文件

```
PaddleOCR-VL-GGUF/
├── demo_ppocrvl_gguf_server.py   # 服务器 (核心)
├── convert_to_gguf.py            # 权重提取
├── 中文说明.md                    # 详细文档
├── 简明教程.md                    # 本文档
└── requirements.txt              # 依赖列表
```

## 🔗 进阶学习

- 详细文档: `中文说明.md`
- 架构说明: `ARCHITECTURE.md`
- 转换指南: `extracted_llm/CONVERSION_README.md` (提取权重后生成)

## 💬 获取帮助

遇到问题?
1. 查看 `中文说明.md` 的故障排除部分
2. 查看 `extracted_llm/CONVERSION_README.md`
3. 提交 GitHub Issue

---

**快速链接**:
- [完整中文文档](中文说明.md)
- [英文文档](README.md)
- [架构详解](ARCHITECTURE.md)

**版本**: v1.0 (llama.cpp 直接版) | **更新**: 2025-11-09
