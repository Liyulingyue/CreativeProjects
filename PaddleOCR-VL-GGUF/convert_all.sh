#!/bin/bash
# PaddleOCR-VL å…¨æµç¨‹è½¬æ¢è„šæœ¬
# ä»åŸå§‹æ¨¡å‹åˆ° GGUF é‡åŒ–æ¨¡å‹çš„å®Œæ•´æµç¨‹

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

# é»˜è®¤å‚æ•°
INPUT_MODEL_PATH="PaddlePaddle/PaddleOCR-VL"
VISION_OUTPUT_PATH="vision_model"
LLM_OUTPUT_PATH="language_model"
GGUF_OUTPUT_PATH="gguf_model"
GGUF_MODEL_PATH="${GGUF_OUTPUT_PATH}/llm_model.gguf"
QUANTIZED_MODEL_PATH="${GGUF_OUTPUT_PATH}/llm_model_q4.gguf"
QUANTIZATION_TYPE="Q4_K_M"

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# æ£€æŸ¥ä¾èµ–
check_dependencies() {
    log_info "æ£€æŸ¥ç³»ç»Ÿä¾èµ–..."

    # æ£€æŸ¥Python
    if ! command -v python &> /dev/null; then
        log_error "Python æœªæ‰¾åˆ°ï¼Œè¯·å®‰è£… Python 3.8+"
        exit 1
    fi

    # æ£€æŸ¥pip
    if ! command -v pip &> /dev/null; then
        log_error "pip æœªæ‰¾åˆ°ï¼Œè¯·å®‰è£… pip"
        exit 1
    fi

    # æ£€æŸ¥git
    if ! command -v git &> /dev/null; then
        log_error "git æœªæ‰¾åˆ°ï¼Œè¯·å®‰è£… git"
        exit 1
    fi

    # æ£€æŸ¥cmake
    if ! command -v cmake &> /dev/null; then
        log_error "cmake æœªæ‰¾åˆ°ï¼Œè¯·å®‰è£… cmake"
        exit 1
    fi

    # æ£€æŸ¥gcc/g++
    if ! command -v gcc &> /dev/null || ! command -v g++ &> /dev/null; then
        log_error "gcc/g++ æœªæ‰¾åˆ°ï¼Œè¯·å®‰è£… build-essential"
        exit 1
    fi

    log_success "ç³»ç»Ÿä¾èµ–æ£€æŸ¥é€šè¿‡"
}

# æ£€æŸ¥è¾“å…¥å‚æ•°
parse_args() {
    while [[ $# -gt 0 ]]; do
        case $1 in
            --input-path)
                INPUT_MODEL_PATH="$2"
                shift 2
                ;;
            --vision-output)
                VISION_OUTPUT_PATH="$2"
                shift 2
                ;;
            --llm-output)
                LLM_OUTPUT_PATH="$2"
                shift 2
                ;;
            --gguf-output)
                GGUF_OUTPUT_PATH="$2"
                shift 2
                ;;
            --quantization-type)
                QUANTIZATION_TYPE="$2"
                shift 2
                ;;
            --help)
                echo "PaddleOCR-VL å…¨æµç¨‹è½¬æ¢è„šæœ¬"
                echo ""
                echo "ç”¨æ³•: $0 [é€‰é¡¹]"
                echo ""
                echo "é€‰é¡¹:"
                echo "  --input-path PATH        è¾“å…¥æ¨¡å‹è·¯å¾„ (é»˜è®¤: PaddlePaddle/PaddleOCR-VL)"
                echo "  --vision-output PATH     è§†è§‰æ¨¡å‹è¾“å‡ºè·¯å¾„ (é»˜è®¤: vision_model)"
                echo "  --llm-output PATH        è¯­è¨€æ¨¡å‹è¾“å‡ºè·¯å¾„ (é»˜è®¤: language_model)"
                echo "  --gguf-output PATH       GGUFæ¨¡å‹è¾“å‡ºè·¯å¾„ (é»˜è®¤: gguf_model)"
                echo "  --quantization-type TYPE é‡åŒ–ç±»å‹ (é»˜è®¤: Q4_K_M)"
                echo "  --help                   æ˜¾ç¤ºæ­¤å¸®åŠ©ä¿¡æ¯"
                echo ""
                echo "ç¤ºä¾‹:"
                echo "  $0"
                echo "  $0 --input-path /path/to/model --quantization-type Q8_0"
                exit 0
                ;;
            *)
                log_error "æœªçŸ¥é€‰é¡¹: $1"
                echo "ä½¿ç”¨ --help æŸ¥çœ‹å¸®åŠ©ä¿¡æ¯"
                exit 1
                ;;
        esac
    done

    # æ›´æ–°è·¯å¾„ç›¸å…³çš„å˜é‡
    GGUF_MODEL_PATH="${GGUF_OUTPUT_PATH}/llm_model.gguf"
    QUANTIZED_MODEL_PATH="${GGUF_OUTPUT_PATH}/llm_model_q4.gguf"
}

# æ£€æŸ¥è¾“å…¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
check_input_model() {
    if [ ! -d "$INPUT_MODEL_PATH" ]; then
        log_error "è¾“å…¥æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: $INPUT_MODEL_PATH"
        log_error "è¯·ç¡®ä¿å·²ä¸‹è½½ PaddleOCR-VL æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„"
        exit 1
    fi

    log_success "è¾“å…¥æ¨¡å‹è·¯å¾„å­˜åœ¨: $INPUT_MODEL_PATH"
}

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
activate_venv() {
    if [ -d ".venv" ]; then
        log_info "æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ..."
        source .venv/bin/activate
    else
        log_warning "æœªæ‰¾åˆ°è™šæ‹Ÿç¯å¢ƒ (.venv)ï¼Œå°†ä½¿ç”¨ç³»ç»Ÿ Python"
    fi
}

# å®‰è£…Pythonä¾èµ–
install_dependencies() {
    log_info "å®‰è£… Python ä¾èµ–..."

    if [ ! -f "requirements.txt" ]; then
        log_error "requirements.txt æ–‡ä»¶ä¸å­˜åœ¨"
        exit 1
    fi

    pip install -r requirements.txt
    pip install llama-cpp-python

    log_success "Python ä¾èµ–å®‰è£…å®Œæˆ"
}

# å¯¼å‡ºè§†è§‰æ¨¡å‹
export_vision_model() {
    log_info "æ­¥éª¤ 1/5: å¯¼å‡ºè§†è§‰æ¨¡å‹..."
    log_info "è¾“å…¥: $INPUT_MODEL_PATH"
    log_info "è¾“å‡º: $VISION_OUTPUT_PATH"

    if [ -d "$VISION_OUTPUT_PATH" ]; then
        log_warning "è§†è§‰æ¨¡å‹è¾“å‡ºç›®å½•å·²å­˜åœ¨ï¼Œè·³è¿‡å¯¼å‡º: $VISION_OUTPUT_PATH"
        return
    fi

    python export_vision_model.py \
        --input-path "$INPUT_MODEL_PATH" \
        --output-path "$VISION_OUTPUT_PATH"

    log_success "è§†è§‰æ¨¡å‹å¯¼å‡ºå®Œæˆ"
}

# å¯¼å‡ºè¯­è¨€æ¨¡å‹
export_language_model() {
    log_info "æ­¥éª¤ 2/5: å¯¼å‡ºè¯­è¨€æ¨¡å‹..."
    log_info "è¾“å…¥: $INPUT_MODEL_PATH"
    log_info "è¾“å‡º: $LLM_OUTPUT_PATH"

    if [ -d "$LLM_OUTPUT_PATH" ]; then
        log_warning "è¯­è¨€æ¨¡å‹è¾“å‡ºç›®å½•å·²å­˜åœ¨ï¼Œè·³è¿‡å¯¼å‡º: $LLM_OUTPUT_PATH"
        return
    fi

    python export_language_model.py \
        --input-path "$INPUT_MODEL_PATH" \
        --output-path "$LLM_OUTPUT_PATH"

    log_success "è¯­è¨€æ¨¡å‹å¯¼å‡ºå®Œæˆ"
}

# ç¼–è¯‘llama.cpp
build_llama_cpp() {
    log_info "æ­¥éª¤ 3/5: ç¼–è¯‘ llama.cpp..."

    if [ -d "llama.cpp" ]; then
        log_info "llama.cpp å·²å­˜åœ¨ï¼Œæ£€æŸ¥æ˜¯å¦å·²ç¼–è¯‘..."
        if [ -f "llama.cpp/bin/llama-quantize" ]; then
            log_success "llama.cpp å·²ç¼–è¯‘ï¼Œè·³è¿‡"
            return
        fi
    else
        log_info "å…‹éš† llama.cpp..."
        git clone https://github.com/ggml-org/llama.cpp
    fi

    log_info "ç¼–è¯‘ llama.cpp..."
    cd llama.cpp
    cmake . -DCMAKE_BUILD_TYPE=Release
    cmake --build . -j$(nproc)
    cd ..

    if [ ! -f "llama.cpp/bin/llama-quantize" ]; then
        log_error "llama.cpp ç¼–è¯‘å¤±è´¥"
        exit 1
    fi

    log_success "llama.cpp ç¼–è¯‘å®Œæˆ"
}

# è½¬æ¢ä¸ºGGUFæ ¼å¼
convert_to_gguf() {
    log_info "æ­¥éª¤ 4/5: è½¬æ¢ä¸º GGUF æ ¼å¼..."
    log_info "è¾“å…¥: $LLM_OUTPUT_PATH/hf_model"
    log_info "è¾“å‡º: $GGUF_MODEL_PATH"

    if [ -f "$GGUF_MODEL_PATH" ]; then
        log_warning "GGUF æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡è½¬æ¢: $GGUF_MODEL_PATH"
        return
    fi

    # åˆ›å»ºè¾“å‡ºç›®å½•
    mkdir -p "$GGUF_OUTPUT_PATH"

    python llama.cpp/convert_hf_to_gguf.py \
        "$LLM_OUTPUT_PATH/hf_model" \
        --outfile "$GGUF_MODEL_PATH" \
        --outtype f16

    if [ ! -f "$GGUF_MODEL_PATH" ]; then
        log_error "GGUF è½¬æ¢å¤±è´¥"
        exit 1
    fi

    log_success "GGUF è½¬æ¢å®Œæˆ"
}

# é‡åŒ–æ¨¡å‹
quantize_model() {
    log_info "æ­¥éª¤ 5/5: é‡åŒ–æ¨¡å‹..."
    log_info "è¾“å…¥: $GGUF_MODEL_PATH"
    log_info "è¾“å‡º: $QUANTIZED_MODEL_PATH"
    log_info "é‡åŒ–ç±»å‹: $QUANTIZATION_TYPE"

    if [ -f "$QUANTIZED_MODEL_PATH" ]; then
        log_warning "é‡åŒ–æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡é‡åŒ–: $QUANTIZED_MODEL_PATH"
        return
    fi

    ./llama.cpp/bin/llama-quantize \
        "$GGUF_MODEL_PATH" \
        "$QUANTIZED_MODEL_PATH" \
        "$QUANTIZATION_TYPE"

    if [ ! -f "$QUANTIZED_MODEL_PATH" ]; then
        log_error "æ¨¡å‹é‡åŒ–å¤±è´¥"
        exit 1
    fi

    log_success "æ¨¡å‹é‡åŒ–å®Œæˆ"
}

# æ˜¾ç¤ºç»“æœ
show_results() {
    echo ""
    echo "========================================"
    log_success "ğŸ‰ å…¨æµç¨‹è½¬æ¢å®Œæˆï¼"
    echo ""
    echo "ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:"
    echo "   è§†è§‰æ¨¡å‹: $VISION_OUTPUT_PATH/"
    echo "   è¯­è¨€æ¨¡å‹: $LLM_OUTPUT_PATH/"
    echo "   GGUF æ¨¡å‹: $GGUF_MODEL_PATH"
    echo "   é‡åŒ–æ¨¡å‹: $QUANTIZED_MODEL_PATH"
    echo ""
    echo "ğŸš€ å¯åŠ¨æœåŠ¡å™¨:"
    echo "   python demo_ppocrvl_gguf_server.py"
    echo ""
    echo "ğŸ§ª æµ‹è¯•å®¢æˆ·ç«¯:"
    echo "   python demo_ppocrvl_gguf_client.py --image test.png"
    echo "========================================"
}

# ä¸»å‡½æ•°
main() {
    echo "========================================"
    echo "ğŸš€ PaddleOCR-VL å…¨æµç¨‹è½¬æ¢è„šæœ¬"
    echo "========================================"

    parse_args "$@"
    check_dependencies
    check_input_model
    activate_venv
    install_dependencies

    # æ‰§è¡Œè½¬æ¢æµç¨‹
    export_vision_model
    export_language_model
    build_llama_cpp
    convert_to_gguf
    quantize_model

    show_results

    log_success "æ‰€æœ‰æ­¥éª¤å®Œæˆï¼"
}

# è¿è¡Œä¸»å‡½æ•°
main "$@"