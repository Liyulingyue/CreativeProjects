# PaddleOCR-VL GGUF (llama.cpp ç‰ˆ)

## é¡¹ç›®æ¦‚è§ˆ

PaddleOCR-VL GGUF é¡¹ç›®å°†å¤šæ¨¡æ€æ¨¡å‹æ‹†åˆ†æˆã€Œè§†è§‰ç¼–ç å™¨ + è¯­è¨€æ¨¡å‹ã€ä¸¤éƒ¨åˆ†,è§†è§‰ä¾§ä¿æŒ PyTorch,è¯­è¨€ä¾§ä½¿ç”¨ GGUF é‡åŒ–åé€šè¿‡ **llama.cpp** ç³»åˆ—å·¥å…·ç›´æ¥åŠ è½½ã€‚

- ğŸ¯ **ç›®æ ‡**: åœ¨æ¶ˆè´¹çº§ç¡¬ä»¶ä¸Šä»¥æœ€å°çš„å†…å­˜å ç”¨å’Œå»¶è¿Ÿè¿è¡Œ PaddleOCR-VL
- ğŸ§  **è§†è§‰ä¾§**: SiglipVisionModel + Projector (åŸç”Ÿç²¾åº¦)
- ğŸ—£ï¸ **è¯­è¨€ä¾§**: Ernie4.5 â†’ GGUF é‡åŒ– â†’ llama-cpp-python æ¨ç†
- ğŸ”Œ **æ¥å£**: å…¼å®¹ OpenAI Chat Completions API
- ğŸ§° **ç”¨é€”**: å›¾ç‰‡ OCR + å¯¹è¯ã€æ–‡æ¡£ç†è§£ç­‰å¤šæ¨¡æ€ä»»åŠ¡

## å…³é”®ç‰¹æ€§

| èƒ½åŠ› | è¯´æ˜ |
|------|------|
| æ¨ç†é€Ÿåº¦ | CPU ç¯å¢ƒä¸‹å¯è·å¾— 2-5x æå‡ |
| æ¶æ„è§£è€¦ | è§†è§‰æ¨¡å—ä»åœ¨ PyTorch ä¸­è¿è¡Œ,ä¾¿äºè°ƒè¯•ä¸æ‰©å±• |
| API å…¼å®¹ | ä¿æŒä¸ OpenAI é£æ ¼æ¥å£ä¸€è‡´,å¯æ— ç¼é›†æˆç°æœ‰åº”ç”¨ |
| æœ¬åœ°åŒ– | å…¨æµç¨‹ç¦»çº¿éƒ¨ç½²,æ— å¤–éƒ¨æœåŠ¡ä¾èµ– |

## é¡¹ç›®ç»“æ„

```
PaddleOCR-VL-GGUF/
â”œâ”€â”€ demo_ppocrvl_gguf_server.py   # llama.cpp åç«¯æœåŠ¡å™¨ (æ ¸å¿ƒ)
â”œâ”€â”€ demo_ppocrvl_gguf_client.py   # å‘½ä»¤è¡Œå®¢æˆ·ç«¯ç¤ºä¾‹
â”œâ”€â”€ convert_to_gguf.py            # æå–ä¸å¯¼å‡º LLM æƒé‡
â”œâ”€â”€ demo_architecture.py          # æ¶æ„å’Œå‚æ•°ç»Ÿè®¡è„šæœ¬
â”œâ”€â”€ requirements.txt              # è¿è¡Œæ‰€éœ€çš„ Python ä¾èµ–
â”œâ”€â”€ README.md                     # æœ¬æ–‡æ¡£ (æ•´åˆç‰ˆ)
â””â”€â”€ PaddlePaddle/
    â””â”€â”€ PaddleOCR-VL/             # å®˜æ–¹ PaddleOCR-VL æƒé‡ (éœ€å•ç‹¬ä¸‹è½½)
```

## ä¸‰æ­¥å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

pip install -r requirements.txt

# å®‰è£… llama-cpp-python (CPU ç‰ˆæœ¬)


# GPU/Metal ç”¨æˆ· (äºŒé€‰ä¸€)
# CUDA: CMAKE_ARGS="-DLLAMA_CUBLAS=on" pip install llama-cpp-python
# Metal: CMAKE_ARGS="-DLLAMA_METAL=on" pip install llama-cpp-python
```

### 2. æå–ä¸é‡åŒ–è¯­è¨€æ¨¡å‹

```bash
# æå– Ernie4.5 ç›¸å…³æƒé‡ (ä¿æŒåœ¨ PaddleOCR-VL-GGUF ç›®å½•ä¸‹ï¼ŒPaddlePaddle/PaddleOCR-VL éœ€é¢„å…ˆä¸‹è½½)
python convert_to_gguf.py \
    --model-path PaddlePaddle/PaddleOCR-VL \
    --output-path extracted_llm \
    --hf-output-dir extracted_llm/hf_model

# ä½¿ç”¨ llama.cpp å°†æƒé‡è½¬æ¢ä¸º GGUF å¹¶é‡åŒ–
# å®‰è£…å¿…è¦çš„ç³»ç»Ÿä¾èµ– (Linux)
sudo apt update && sudo apt install -y libcurl4-openssl-dev

git clone https://github.com/ggml-org/llama.cpp
cd llama.cpp && cmake . && cmake --build . -j$(nproc) && cd ..

# ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒä¸­çš„ Python è¿è¡Œè½¬æ¢è„šæœ¬
python llama.cpp/convert_hf_to_gguf.py \
  extracted_llm/hf_model \
  --outfile extracted_llm/llm_model.gguf \
  --outtype f16

# ä½¿ç”¨ç¼–è¯‘åçš„äºŒè¿›åˆ¶è¿›è¡Œé‡åŒ–
./llama.cpp/bin/llama-quantize extracted_llm/llm_model.gguf \
                              extracted_llm/llm_model_q4.gguf Q4_K_M
```

### 3. å¯åŠ¨æœåŠ¡å¹¶æµ‹è¯•

```bash
# ç»ˆç«¯ 1: å¯åŠ¨å¤šæ¨¡æ€æœåŠ¡
# in PaddleOCR-VL-GGUF
python demo_ppocrvl_gguf_server.py

# ç»ˆç«¯ 2: å‘é€æµ‹è¯•è¯·æ±‚
python demo_ppocrvl_gguf_client.py \
    --image /path/to/image.jpg
```

## å…¶ä»–è¯´æ˜

### æå–è¯­è¨€æ¨¡å‹æƒé‡

`convert_to_gguf.py` ä¼šåœ¨ `extracted_llm/` ç›®å½•ä¸‹ç”Ÿæˆä»¥ä¸‹æ–‡ä»¶:

- `llm_model.pt` / `lm_head.pt`: PyTorch æƒé‡
- `llm_config.json`: é…ç½®æ–‡ä»¶,ä¾›åç»­è½¬æ¢è„šæœ¬ä½¿ç”¨
- `hf_model/`: å¯ç›´æ¥ç»™ `convert_hf_to_gguf.py` ä½¿ç”¨çš„ Hugging Face æ£€æŸ¥ç‚¹ (å¦‚éœ€å…³é—­å¯æ·»åŠ  `--no-hf-export`)

### ä½¿ç”¨ llama.cpp è½¬æ¢ä¸é‡åŒ–

1. å…‹éš†å¹¶ç¼–è¯‘ llama.cpp (å¯é€‰ GPU æ”¯æŒ)
2. ç¼–å†™/é€‚é… `convert.py` å°†æå–çš„æƒé‡è½¬ä¸º GGUF
3. é€šè¿‡ `quantize` è¿›è¡Œé‡åŒ–,æ¨è `Q4_K_M`

å¸¸ç”¨å‘½ä»¤:

```bash
./quantize input.gguf output_q4.gguf Q4_K_M
./quantize input.gguf output_q5.gguf Q5_K_M
```

é‡åŒ–ç­‰çº§å»ºè®®:

| ç­‰çº§ | å†…å­˜ | è´¨é‡ | å¤‡æ³¨ |
|------|------|------|------|
| Q4_0 | æœ€ä½ | è¾ƒä½ | è°ƒè¯•ä¸åŸå‹ |
| **Q4_K_M** | ä½ | ä½³ | é»˜è®¤æ¨è |
| Q5_K_M | ä¸­ | å¾ˆå¥½ | è´¨é‡ä¼˜å…ˆ |
| Q8_0 | é«˜ | æ¥è¿‘ FP16 | é«˜ç²¾åº¦éœ€æ±‚ |

### é…ç½®æœåŠ¡å™¨å‚æ•°

`demo_ppocrvl_gguf_server.py` ä¸­çš„å…³é”®å‚æ•°:

```python
GGUF_MODEL_PATH = "extracted_llm/llm_model_q4.gguf"  # GGUF æ¨¡å‹è·¯å¾„
N_GPU_LAYERS = 0     # GPU å±‚æ•° (0=çº¯ CPU, é€‚å½“å¢å¤§å¯ç”¨ GPU åŠ é€Ÿ)
N_CTX = 4096         # ä¸Šä¸‹æ–‡çª—å£
N_THREADS = 8        # CPU çº¿ç¨‹æ•°,å»ºè®®ä¸ç‰©ç†æ ¸å¿ƒæ•°åŒ¹é…
```

GPU ç”¨æˆ·å¯æ ¹æ®æ˜¾å­˜è®¾ç½® `N_GPU_LAYERS` (ä¾‹å¦‚ 32 æˆ–æ›´é«˜)ã€‚

### API è°ƒç”¨ç¤ºä¾‹

```python
import requests

payload = {
    "messages": [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "è¯†åˆ«å›¾ç‰‡ä¸­çš„æ–‡å­—"},
                {"type": "image_url", "image_url": {"url": "data:image/jpeg;base64,..."}}
            ]
        }
    ],
    "max_tokens": 1024,
    "temperature": 0.7,
    "stream": False
}

response = requests.post(
    "http://localhost:7778/v1/chat/completions",
    json=payload,
    timeout=120
)

print(response.json()["choices"][0]["message"]["content"])
```

## æ¶æ„è¯´æ˜

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 PaddleOCR-VL GGUF æ··åˆæ¶æ„                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¾“å…¥å›¾åƒ + æ–‡æœ¬æç¤º                                         â”‚
â”‚    â”‚                                                        â”‚
â”‚    â–¼                                                        â”‚
â”‚ è§†è§‰ç¼–ç å™¨ (PyTorch)                                        â”‚
â”‚  â”œâ”€ SiglipVisionModel                                     â”‚
â”‚  â””â”€ Attention Pooling                                        â”‚
â”‚    â”‚                                                        â”‚
â”‚    â–¼                                                        â”‚
â”‚ Projector (PyTorch)                                        â”‚
â”‚    â”‚                                                        â”‚
â”‚    â–¼ è§†è§‰åµŒå…¥                                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ llama-cpp æ¨ç† (GGUF)                                       â”‚
â”‚  â”œâ”€ Ernie4.5 Decoder                                      â”‚
â”‚  â””â”€ LM Head                                               â”‚
â”‚    â”‚                                                        â”‚
â”‚    â–¼                                                        â”‚
â”‚ ç”Ÿæˆæ–‡æœ¬è¾“å‡º                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

> è§†è§‰éƒ¨åˆ†ä»ä¿æŒåŸå§‹ç²¾åº¦,ä¸»è¦çš„æ€§èƒ½ä¸å†…å­˜ä¼˜åŒ–é›†ä¸­åœ¨ LLM ä¾§ã€‚

## è´¡çŒ®ä¸è®¸å¯è¯

- ğŸš€ æ¬¢è¿é€šè¿‡ Issue/PR æäº¤æ”¹è¿›å»ºè®®æˆ–è¡¥å……è½¬æ¢è„šæœ¬
- ğŸ“„ è®¸å¯è¯éµå¾ª PaddleOCR-VL åŸé¡¹ç›®,è¯¦æƒ…å‚è§ä»“åº“æ ¹ç›®å½• `LICENSE`

## å‚è€ƒèµ„æº

- [PaddleOCR å®˜æ–¹ä»“åº“](https://github.com/PaddlePaddle/PaddleOCR)
- [llama.cpp](https://github.com/ggml-org/llama.cpp)
- [llama-cpp-python](https://github.com/abetlen/llama-cpp-python)
- [GGUF æ ¼å¼è¯´æ˜](https://github.com/ggerganov/ggml/blob/master/docs/gguf.md)

å¦‚é‡é—®é¢˜,è¯·åœ¨ GitHub Issues ä¸­åé¦ˆæˆ–æäº¤è®¨è®ºã€‚


## è¿è¡Œè€—æ—¶æµ‹è¯•
|è®¾å¤‡ | å›¾ç‰‡å°ºå¯¸ | è€—æ—¶(ç§’) | é‡åŒ–ç­‰çº§ |
|----|---------|---------|-----|
| RDK X5(8x A55@1.5GHz, 4Gå†…å­˜ç‰ˆæœ¬) | 256Ã—256 | 45 | Q4_K_M |
| RDK X5(8x A55@1.5GHz, 4Gå†…å­˜ç‰ˆæœ¬) | 640x480 | 97.06 | Q4_K_M |
| Intel Ultra5 | 256Ã—256 | 4.55 | Q4_K_M |
| Intel Ultra5 | 640x480 | 8.59 | Q4_K_M |

## é‡åŒ–å‰è€—æ—¶æµ‹è¯•
|è®¾å¤‡ | å›¾ç‰‡å°ºå¯¸ | è€—æ—¶(ç§’) |
|----|---------|---------|
| RDK X5(8x A55@1.5GHz, 4Gå†…å­˜ç‰ˆæœ¬) | 256Ã—256 | 154.66 |
| RDK X5(8x A55@1.5GHz, 4Gå†…å­˜ç‰ˆæœ¬) | 640x480 | 435 |
| Intel 13th Gen Intel(R) Core(TM) i7-13700K | 256Ã—256 | 7.3 |
| Intel 13th Gen Intel(R) Core(TM) i7-13700K | 640x480 | 13.25 |

## æœåŠ¡ç«¯ä»£ç æ ·ä¾‹
```python
# PaddleOCR-VL with GGUF LLM Backend
# è§†è§‰ç¼–ç å™¨éƒ¨åˆ†ä½¿ç”¨ PyTorchï¼ŒLLM éƒ¨åˆ†ä½¿ç”¨ llama.cpp/GGUF åŠ é€Ÿ

import base64
from io import BytesIO
import ctypes
import os
import torch
from PIL import Image
from transformers import AutoProcessor
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
import uvicorn
import requests
import time
import json
import gc
import traceback
import numpy as np

# ä½¿ç”¨ llama-cpp-python ç›´æ¥åŠ è½½ GGUF æ¨¡å‹
try:
    from llama_cpp import Llama
    from llama_cpp import llama_cpp as llama_cpp_lib
    LLAMA_CPP_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: llama-cpp-python æœªå®‰è£…")
    print("è¯·è¿è¡Œ: pip install llama-cpp-python")
    LLAMA_CPP_AVAILABLE = False
    llama_cpp_lib = None

LOCAL_PATH = "PaddlePaddle/PaddleOCR-VL"  # è§†è§‰æ¨¡å‹è·¯å¾„
GGUF_MODEL_PATH = "extracted_llm/llm_model_q4.gguf"  # GGUF æ¨¡å‹è·¯å¾„
N_GPU_LAYERS = 0  # GPU å±‚æ•°ï¼Œ0 è¡¨ç¤ºçº¯ CPU
N_CTX = 4096  # ä¸Šä¸‹æ–‡é•¿åº¦
N_THREADS = 8  # CPU çº¿ç¨‹æ•°

print(f"=== PaddleOCR-VL GGUF API å¯åŠ¨ä¸­ ===")
print(f"è§†è§‰æ¨¡å‹è·¯å¾„: {LOCAL_PATH}")
print(f"LLM åç«¯: llama.cpp (ç›´æ¥)")
print(f"GGUF æ¨¡å‹è·¯å¾„: {GGUF_MODEL_PATH}")

try:
    # åªåŠ è½½ processor å’Œè§†è§‰æ¨¡å‹éƒ¨åˆ†
    processor = AutoProcessor.from_pretrained(LOCAL_PATH, trust_remote_code=True, use_fast=True)
    
    # åŠ è½½å®Œæ•´æ¨¡å‹ç”¨äºæå–è§†è§‰ç¼–ç å™¨
    from transformers import AutoModelForCausalLM
    print("æ­£åœ¨åŠ è½½å®Œæ•´æ¨¡å‹ä»¥æå–è§†è§‰ç¼–ç å™¨...")
    full_model = AutoModelForCausalLM.from_pretrained(
        LOCAL_PATH, 
        trust_remote_code=True, 
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True
    ).to("cpu")
    
    # æå–è§†è§‰ç¼–ç å™¨å’ŒæŠ•å½±å±‚
    visual_encoder = full_model.visual
    projector = full_model.mlp_AR
    
    # æ¸…ç†å®Œæ•´æ¨¡å‹ï¼Œåªä¿ç•™éœ€è¦çš„éƒ¨åˆ†
    del full_model.model  # åˆ é™¤ LLM éƒ¨åˆ†
    del full_model.lm_head
    del full_model
    gc.collect()
    
    visual_encoder.eval()
    projector.eval()
    
    print("è§†è§‰ç¼–ç å™¨åŠ è½½æˆåŠŸ")
    
    # åŠ è½½ GGUF LLM æ¨¡å‹
    llm_model = None
    if LLAMA_CPP_AVAILABLE:
        if os.path.exists(GGUF_MODEL_PATH):
            print(f"æ­£åœ¨åŠ è½½ GGUF æ¨¡å‹: {GGUF_MODEL_PATH}")
            llm_model = Llama(
                model_path=GGUF_MODEL_PATH,
                n_gpu_layers=N_GPU_LAYERS,
                n_ctx=N_CTX,
                n_threads=N_THREADS,
                verbose=False
            )
            print("GGUF æ¨¡å‹åŠ è½½æˆåŠŸ")
            print(f"  - GPU å±‚æ•°: {N_GPU_LAYERS}")
            print(f"  - ä¸Šä¸‹æ–‡é•¿åº¦: {N_CTX}")
            print(f"  - CPU çº¿ç¨‹æ•°: {N_THREADS}")
        else:
            print(f"è­¦å‘Š: GGUF æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {GGUF_MODEL_PATH}")
            print("LLM æ¨ç†å°†æ— æ³•å·¥ä½œï¼Œè¯·å…ˆè¿è¡Œ convert_to_gguf.py å¹¶å®Œæˆ GGUF è½¬æ¢")
    else:
        print("è­¦å‘Š: llama-cpp-python æœªå®‰è£…ï¼ŒLLM æ¨ç†å°†æ— æ³•å·¥ä½œ")
    
except Exception as e:
    print(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    traceback.print_exc()
    raise

IMAGE_PLACEHOLDER_TOKEN = "<|IMAGE_PLACEHOLDER|>"
try:
    IMAGE_PLACEHOLDER_ID = processor.tokenizer.convert_tokens_to_ids(IMAGE_PLACEHOLDER_TOKEN)
except Exception:
    IMAGE_PLACEHOLDER_ID = None

DEFAULT_STOP_STRINGS = ["</s>", "<|im_end|>", "<|end|>"]
STOP_TOKEN_IDS_FROM_TOKENIZER = set()
try:
    for _stop_token in DEFAULT_STOP_STRINGS:
        token_id = processor.tokenizer.convert_tokens_to_ids(_stop_token)
        if token_id is not None and token_id != getattr(processor.tokenizer, "unk_token_id", None):
            STOP_TOKEN_IDS_FROM_TOKENIZER.add(int(token_id))
except Exception:
    STOP_TOKEN_IDS_FROM_TOKENIZER = set()

app = FastAPI()


def _apply_stop_sequences(text: str, stop_strings):
    """Trim text at the earliest occurrence of any stop string."""
    earliest_idx = None
    for stop in stop_strings or []:
        if not stop:
            continue
        idx = text.find(stop)
        if idx != -1 and (earliest_idx is None or idx < earliest_idx):
            earliest_idx = idx
    if earliest_idx is not None:
        return text[:earliest_idx], True
    return text, False


def _longest_partial_stop_suffix(text: str, stop_strings):
    """Return length of the longest suffix that is a prefix of any stop string."""
    max_len = 0
    for stop in stop_strings or []:
        if not stop:
            continue
        max_check = min(len(stop) - 1, len(text))
        for length in range(max_check, 0, -1):
            if text.endswith(stop[:length]) and length < len(stop):
                if length > max_len:
                    max_len = length
                break
    return max_len


def _build_stop_token_ids(llm):
    stop_ids = set(STOP_TOKEN_IDS_FROM_TOKENIZER)
    try:
        eos_id = llm.token_eos()
        if eos_id is not None and eos_id >= 0:
            stop_ids.add(int(eos_id))
    except Exception:
        pass
    try:
        sep_id = llm._model.token_sep()
        if sep_id is not None and sep_id >= 0:
            stop_ids.add(int(sep_id))
    except Exception:
        pass
    return {sid for sid in stop_ids if sid is not None and sid >= 0}


def _inject_embedding_token(llm, embedding_vector, placeholder_token_id):
    if not LLAMA_CPP_AVAILABLE:
        raise RuntimeError("llama.cpp åç«¯ä¸å¯ç”¨")
    embedding_view = np.asarray(embedding_vector, dtype=np.float32)
    if embedding_view.ndim != 1:
        embedding_view = embedding_view.reshape(-1)
    n_embd = llm.n_embd()
    if embedding_view.shape[0] != n_embd:
        raise ValueError(
            f"å›¾åƒåµŒå…¥ç»´åº¦ {embedding_view.shape[0]} ä¸æ¨¡å‹éšè—ç»´åº¦ {n_embd} ä¸åŒ¹é…"
        )
    batch = llama_cpp_lib.llama_batch_init(1, n_embd, 1)
    try:
        float_ptr = ctypes.cast(batch.embd, ctypes.POINTER(ctypes.c_float))
        np_view = np.ctypeslib.as_array(float_ptr, shape=(n_embd,))
        np_view[:] = embedding_view

        batch.n_tokens = 1
        pos = llm.n_tokens
        batch.pos[0] = pos
        batch.n_seq_id[0] = 1
        batch.seq_id[0][0] = 0
        batch.logits[0] = 1
        token_ptr = getattr(batch, "token", None)
        if token_ptr:
            token_array = ctypes.cast(token_ptr, ctypes.POINTER(ctypes.c_int32))
            if placeholder_token_id is not None and placeholder_token_id >= 0:
                token_array[0] = int(placeholder_token_id)
            else:
                try:
                    token_array[0] = llama_cpp_lib.llama_token_null()
                except AttributeError:
                    token_array[0] = -1

        rc = llama_cpp_lib.llama_decode(llm._ctx.ctx, batch)
        if rc != 0:
            raise RuntimeError(f"llama_decode è¿”å›é”™è¯¯ç  {rc}")

        token_to_store = placeholder_token_id if placeholder_token_id is not None else -1
        llm.input_ids[pos] = token_to_store
        if getattr(llm, "_logits_all", False):
            cols = llm._n_vocab
            logits = np.ctypeslib.as_array(llm._ctx.get_logits(), shape=(cols,))
            llm.scores[pos : pos + 1, :].reshape(-1)[:] = logits
        llm.n_tokens += 1
    finally:
        llama_cpp_lib.llama_batch_free(batch)


def _apply_prompt_tokens(llm, prompt_tokens, image_embeds):
    if image_embeds is None or len(image_embeds) == 0:
        llm.eval([int(t) for t in prompt_tokens])
        return

    if IMAGE_PLACEHOLDER_ID is None:
        print("è­¦å‘Š: æœªæ‰¾åˆ°å›¾åƒå ä½ç¬¦ token, æŒ‰çº¯æ–‡æœ¬æç¤ºå¤„ç†", flush=True)
        llm.eval([int(t) for t in prompt_tokens])
        return

    placeholder_id = int(IMAGE_PLACEHOLDER_ID)
    token_buffer = []
    embed_idx = 0
    placeholder_hits = 0
    embed_count = len(image_embeds)

    for token in prompt_tokens:
        token_int = int(token)
        if token_int == placeholder_id:
            placeholder_hits += 1
            if token_buffer:
                llm.eval(token_buffer)
                token_buffer = []
            if embed_idx >= embed_count:
                # æ²¡æœ‰å¯¹åº”åµŒå…¥, å›é€€ä¸ºæ™®é€š token
                llm.eval([token_int])
            else:
                _inject_embedding_token(llm, image_embeds[embed_idx], placeholder_id)
                embed_idx += 1
        else:
            token_buffer.append(token_int)

    if token_buffer:
        llm.eval(token_buffer)

    if placeholder_hits != embed_count:
        print(
            f"è­¦å‘Š: å›¾åƒå ä½ç¬¦æ•°é‡ ({placeholder_hits}) ä¸åµŒå…¥æ•°é‡ ({embed_count}) ä¸ä¸€è‡´",
            flush=True,
        )


def _prepare_context(llm, prompt_tokens, image_embeds):
    llm.reset()
    llm._sampler = None
    _apply_prompt_tokens(llm, prompt_tokens, image_embeds)


def _generate_non_stream_completion(llm, prompt_tokens, max_tokens, temperature, stop_strings):
    stop_token_ids = _build_stop_token_ids(llm)
    generator = llm.generate(
        [],
        top_p=0.95,
        temp=temperature,
        repeat_penalty=1.0,
        reset=False,
    )
    prompt_bytes = llm.detokenize(prompt_tokens)
    prompt_text = prompt_bytes.decode("utf-8", errors="ignore")

    generated_tokens = []
    full_output = ""
    stop_hit = False
    length_hit = False

    try:
        for token in generator:
            token_id = int(token)
            if token_id in stop_token_ids or llama_cpp_lib.llama_token_is_eog(llm._model.vocab, token_id):
                stop_hit = True
                break

            generated_tokens.append(token_id)
            all_text = llm.detokenize(prompt_tokens + generated_tokens).decode(
                "utf-8", errors="ignore"
            )
            candidate_output = all_text[len(prompt_text) :]
            candidate_output, triggered = _apply_stop_sequences(
                candidate_output, stop_strings
            )
            full_output = candidate_output

            if triggered:
                stop_hit = True
                break
            if len(generated_tokens) >= max_tokens:
                length_hit = True
                break
    finally:
        generator.close()
        llama_cpp_lib.llama_kv_self_clear(llm._ctx.ctx)
        llm.reset()
        llm._sampler = None

    finish_reason = "stop" if stop_hit else ("length" if length_hit else "stop")
    return full_output, generated_tokens, finish_reason


def _stream_completion(llm, prompt_tokens, max_tokens, temperature, stop_strings, completion_id, created_time, model_name):
    stop_token_ids = _build_stop_token_ids(llm)
    generator = llm.generate(
        [],
        top_p=0.95,
        temp=temperature,
        repeat_penalty=1.0,
        reset=False,
    )
    prompt_bytes = llm.detokenize(prompt_tokens)
    prompt_text = prompt_bytes.decode("utf-8", errors="ignore")

    full_output = ""
    buffered_suffix = ""
    generated_tokens = []
    first_chunk = True
    stop_hit = False
    length_hit = False

    def event_iterator():
        nonlocal full_output, buffered_suffix, first_chunk, stop_hit, length_hit
        try:
            for token in generator:
                token_id = int(token)
                if token_id in stop_token_ids or llama_cpp_lib.llama_token_is_eog(llm._model.vocab, token_id):
                    stop_hit = True
                    break

                generated_tokens.append(token_id)
                all_text = llm.detokenize(prompt_tokens + generated_tokens).decode(
                    "utf-8", errors="ignore"
                )
                candidate_output = all_text[len(prompt_text) :]
                candidate_output, triggered = _apply_stop_sequences(
                    candidate_output, stop_strings
                )
                if triggered:
                    stop_hit = True

                delta_full = candidate_output[len(full_output) :]
                full_output = candidate_output
                if delta_full:
                    delta_full = buffered_suffix + delta_full
                    buffered_suffix = ""

                pending = _longest_partial_stop_suffix(full_output, stop_strings)
                if pending:
                    if len(delta_full) >= pending:
                        buffered_suffix = delta_full[-pending:]
                        delta_to_emit = delta_full[:-pending]
                    else:
                        buffered_suffix = delta_full
                        delta_to_emit = ""
                else:
                    delta_to_emit = delta_full

                if delta_to_emit:
                    delta_payload = {"content": delta_to_emit}
                    if first_chunk:
                        delta_payload["role"] = "assistant"
                    chunk = {
                        "id": completion_id,
                        "object": "chat.completion.chunk",
                        "created": created_time,
                        "model": model_name,
                        "choices": [
                            {
                                "index": 0,
                                "delta": delta_payload,
                                "finish_reason": None,
                            }
                        ],
                    }
                    yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
                    first_chunk = False

                if stop_hit or len(generated_tokens) >= max_tokens:
                    if len(generated_tokens) >= max_tokens and not stop_hit:
                        length_hit = True
                    break

            finish_reason = "stop" if stop_hit else ("length" if length_hit else "stop")
            final_chunk = {
                "id": completion_id,
                "object": "chat.completion.chunk",
                "created": created_time,
                "model": model_name,
                "choices": [
                    {
                        "index": 0,
                        "delta": {},
                        "finish_reason": finish_reason,
                    }
                ],
            }
            yield f"data: {json.dumps(final_chunk, ensure_ascii=False)}\n\n"
            yield "data: [DONE]\n\n"
        finally:
            generator.close()
            llama_cpp_lib.llama_kv_self_clear(llm._ctx.ctx)
            llm.reset()
            llm._sampler = None

    return event_iterator

@app.get("/v1/models")
async def list_models():
    return {
        "object": "list",
        "data": [
            {
                "id": "paddleocr-vl-gguf",
                "object": "model",
                "created": int(time.time()),
                "owned_by": "custom",
                "permission": [],
                "root": "paddleocr-vl-gguf",
                "parent": None,
                "capabilities": {
                    "vision": True,
                    "function_calling": False,
                    "fine_tuning": False,
                    "backend": "llama.cpp-gguf"
                }
            }
        ]
    }

async def encode_vision(image, text_prompt):
    """
    ä½¿ç”¨ PyTorch è§†è§‰ç¼–ç å™¨å¤„ç†å›¾åƒ
    è¿”å›è§†è§‰ç‰¹å¾åµŒå…¥
    """
    try:
        # æ„å»ºæ¶ˆæ¯æ ¼å¼
        content = []
        if image:
            content.append({"type": "image"})
        content.append({"type": "text", "text": text_prompt})
        
        chat_messages = [{"role": "user", "content": content}]
        
        # åº”ç”¨ chat template
        try:
            prompt = processor.tokenizer.apply_chat_template(
                chat_messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
        except Exception as e:
            print(f"Chat template å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ ¼å¼: {e}")
            cls_token = "<|begin_of_sentence|>"
            if image:
                prompt = f"{cls_token}User: <|IMAGE_START|><|IMAGE_PLACEHOLDER|><|IMAGE_END|>{text_prompt}\nAssistant: "
            else:
                prompt = f"{cls_token}User: {text_prompt}\nAssistant: "
        
        # å¤„ç†è¾“å…¥
        inputs = processor(text=prompt, images=image, return_tensors="pt").to("cpu")
        
        input_ids = inputs["input_ids"]
        image_embeds = None

        if image and "pixel_values" in inputs:
            pixel_values = inputs["pixel_values"].unsqueeze(0)
            device = pixel_values.device
            if "image_grid_thw" in inputs:
                grid_values = inputs["image_grid_thw"][0].tolist()
                image_grid_thw = [tuple(int(x) for x in grid_values)]
                token_count = int(np.prod(image_grid_thw[0]))
                siglip_position_ids = torch.arange(token_count, dtype=torch.long, device=device)
                cu_seqlens = torch.tensor([0, token_count], dtype=torch.int32, device=device)
                sample_indices = torch.zeros(token_count, dtype=torch.long, device=device)

                with torch.no_grad():
                    vision_outputs = visual_encoder(
                        pixel_values=pixel_values,
                        image_grid_thw=image_grid_thw,
                        position_ids=siglip_position_ids,
                        vision_return_embed_list=True,
                        interpolate_pos_encoding=True,
                        sample_indices=sample_indices,
                        cu_seqlens=cu_seqlens,
                        return_pooler_output=False,
                        use_rope=True,
                        window_size=-1,
                    )

                    hidden_states = vision_outputs.last_hidden_state
                    if isinstance(hidden_states, list):
                        hidden_states_for_projector = [hs.cpu() for hs in hidden_states]
                    else:
                        hidden_states_for_projector = hidden_states.cpu()

                    projected = projector(hidden_states_for_projector, image_grid_thw)
                    if isinstance(projected, list):
                        projected = torch.cat(projected, dim=0)
                    image_embeds = projected.cpu().numpy()

        return {
            "prompt": prompt,
            "image_embeds": image_embeds,
            "input_ids": input_ids.cpu().numpy()
        }
        
    except Exception as e:
        print(f"è§†è§‰ç¼–ç å¤±è´¥: {e}")
        traceback.print_exc()
        raise

def call_llama_cpp_generate(
    prompt_tokens,
    image_embeds=None,
    *,
    max_tokens=131072,
    temperature=0.7,
    stream=False,
    completion_id=None,
    created_time=None,
    model_name="paddleocr-vl-gguf",
):
    if llm_model is None or not LLAMA_CPP_AVAILABLE:
        raise HTTPException(status_code=500, detail="GGUF æ¨¡å‹æœªåŠ è½½æˆ– llama.cpp ä¸å¯ç”¨")

    if prompt_tokens is None:
        raise HTTPException(status_code=500, detail="ç¼ºå°‘ token åŒ–åçš„æç¤ºä¿¡æ¯")

    prompt_tokens_list = [int(t) for t in prompt_tokens]
    embeds_array = None
    if image_embeds is not None:
        embeds_array = np.asarray(image_embeds, dtype=np.float32)
        if embeds_array.ndim == 1:
            embeds_array = embeds_array.reshape(1, -1)

    if stream:
        _prepare_context(llm_model, prompt_tokens_list, embeds_array)
        return _stream_completion(
            llm_model,
            prompt_tokens_list,
            max_tokens,
            temperature,
            DEFAULT_STOP_STRINGS,
            completion_id or f"chatcmpl-{int(time.time())}",
            created_time or int(time.time()),
            model_name,
        )

    _prepare_context(llm_model, prompt_tokens_list, embeds_array)

    try:
        text, generated_tokens, finish_reason = _generate_non_stream_completion(
            llm_model,
            prompt_tokens_list,
            max_tokens,
            temperature,
            DEFAULT_STOP_STRINGS,
        )
        completion_tokens = len(generated_tokens)
        usage = {
            "prompt_tokens": len(prompt_tokens_list),
            "completion_tokens": completion_tokens,
            "total_tokens": len(prompt_tokens_list) + completion_tokens,
        }
        return {
            "text": text,
            "generated_tokens": generated_tokens,
            "finish_reason": finish_reason,
            "usage": usage,
        }
    except Exception as e:
        print(f"llama.cpp ç”Ÿæˆå¤±è´¥: {e}")
        traceback.print_exc()
        raise

@app.post("/v1/chat/completions")
async def chat_completions(body: dict):
    try:
        messages = body.get("messages", [])
        if not messages:
            raise HTTPException(status_code=400, detail="è¯·æ±‚ä½“ä¸­ç¼ºå°‘messageså­—æ®µ")

        content = messages[-1].get("content", [])
        if isinstance(content, str):
            text_prompt = content
            image_urls = []
        else:
            # æå–å›¾åƒå’Œæ–‡æœ¬
            image_urls = [c["image_url"]["url"] for c in content if c["type"] == "image_url"]
            text_parts = [c["text"] for c in content if c["type"] == "text"]
            text_prompt = " ".join(text_parts) or "Parse the document."
    except KeyError as e:
        print(f"è¯·æ±‚æ ¼å¼é”™è¯¯: {e}")
        traceback.print_exc()
        raise HTTPException(status_code=400, detail=f"è¯·æ±‚æ ¼å¼é”™è¯¯: {e}")

    print(f"æ¥æ”¶åˆ°è¯·æ±‚: æ–‡æœ¬='{text_prompt}', å›¾åƒæ•°é‡={len(image_urls)}")

    # åŠ è½½å›¾åƒ
    images = []
    for idx, url in enumerate(image_urls):
        img = None
        try:
            if url.startswith("data:"):
                if "," in url:
                    _, b64_data = url.split(",", 1)
                else:
                    b64_data = url.replace("data:image/", "").split(";")[0]
                
                img_bytes = base64.b64decode(b64_data)
                img = Image.open(BytesIO(img_bytes))
            else:
                response = requests.get(url, timeout=10)
                response.raise_for_status()
                img = Image.open(BytesIO(response.content))
        except Exception as e:
            print(f"å›¾ç‰‡å¤„ç†å¤±è´¥: {e}")
            traceback.print_exc()
            raise HTTPException(status_code=400, detail=f"å›¾ç‰‡å¤„ç†å¤±è´¥: {e}")

        if img:
            if img.mode != 'RGB':
                img = img.convert('RGB')
            images.append(img)

    image = images[0] if images else None

    # æ­¥éª¤1: ä½¿ç”¨è§†è§‰ç¼–ç å™¨å¤„ç†
    print("æ­¥éª¤1: è§†è§‰ç¼–ç ...")
    vision_result = await encode_vision(image, text_prompt)
    prompt = vision_result["prompt"]
    image_embeds = vision_result["image_embeds"]
    input_ids_array = vision_result.get("input_ids")
    prompt_tokens = None
    if input_ids_array is not None:
        ids_np = np.asarray(input_ids_array)
        if ids_np.ndim == 2:
            prompt_tokens = [int(x) for x in ids_np[0].tolist()]
        else:
            prompt_tokens = [int(x) for x in ids_np.tolist()]
    if prompt_tokens is None:
        raise HTTPException(status_code=500, detail="æœªèƒ½è·å– prompt çš„ token åºåˆ—")
    
    # å…³é—­å›¾ç‰‡é‡Šæ”¾å†…å­˜
    for img in images:
        try:
            img.close()
        except:
            pass
    
    # æ­¥éª¤2: è°ƒç”¨ llama.cpp ç”Ÿæˆ
    print("æ­¥éª¤2: LLM ç”Ÿæˆ...")
    max_tokens = body.get("max_tokens", 1024)  # é™ä½é»˜è®¤å€¼
    temperature = body.get("temperature", 0.7)
    stream = body.get("stream", False)
    
    completion_id = f"chatcmpl-{int(time.time())}"
    created_time = int(time.time())
    
    if stream:
        try:
            event_stream = call_llama_cpp_generate(
                prompt_tokens,
                image_embeds,
                max_tokens=max_tokens,
                temperature=temperature,
                stream=True,
                completion_id=completion_id,
                created_time=created_time,
                model_name="paddleocr-vl-gguf",
            )
        except Exception as e:
            print(f"æµå¼ç”Ÿæˆå¤±è´¥: {e}")
            traceback.print_exc()
            raise
        return StreamingResponse(event_stream, media_type="text/event-stream")

    # éæµå¼
    result = call_llama_cpp_generate(
        prompt_tokens,
        image_embeds,
        max_tokens=max_tokens,
        temperature=temperature,
        stream=False,
        completion_id=completion_id,
        created_time=created_time,
    )
    generated = result.get("text", "")
    finish_reason = result.get("finish_reason", "stop")
    usage_stats = result.get("usage", {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0})

    print(f"ç”Ÿæˆå†…å®¹: {generated[:200]}{'...' if len(generated) > 200 else ''}")

    response = {
        "id": completion_id,
        "object": "chat.completion",
        "created": created_time,
        "model": "paddleocr-vl-gguf",
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": generated,
                },
                "finish_reason": finish_reason,
            }
        ],
        "usage": usage_stats,
    }

    return response

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=7778)  # ä½¿ç”¨ä¸åŒç«¯å£é¿å…å†²çª

```

## å®¢æˆ·ç«¯è°ƒç”¨æ ·ä¾‹
```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PaddleOCR-VL GGUF ç‰ˆæœ¬æµ‹è¯•å®¢æˆ·ç«¯
"""

import sys
import os
import time

# å¤ç”¨ CPU ç‰ˆæœ¬å®¢æˆ·ç«¯å®ç°
cpu_client_dir = os.path.join(os.path.dirname(__file__), "..", "PaddleOCR-VL-CPU")
sys.path.insert(0, os.path.abspath(cpu_client_dir))
from demo_ppocrvl_client import PaddleOCRVLClient
import argparse


def main():
    parser = argparse.ArgumentParser(description="PaddleOCR-VL GGUF API å®¢æˆ·ç«¯æµ‹è¯•")
    parser.add_argument("--url", default="http://localhost:7778", help="APIæœåŠ¡å™¨URL (GGUFç‰ˆæœ¬ä½¿ç”¨7778ç«¯å£)")
    parser.add_argument("--text", default="OCR:", help="æ–‡æœ¬æç¤º")
    parser.add_argument("--image", help="å›¾åƒæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--max-tokens", type=int, default=1024, help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--temperature", type=float, default=0.7, help="æ¸©åº¦å‚æ•°")
    parser.add_argument("--stream", action="store_true", help="å¯ç”¨æµå¼å“åº”")
    parser.add_argument("--list-models", action="store_true", help="åˆ—å‡ºå¯ç”¨æ¨¡å‹")

    args = parser.parse_args()

    client = PaddleOCRVLClient(args.url)

    try:
        if args.list_models:
            models = client.list_models()
            print("å¯ç”¨æ¨¡å‹:")
            for model in models.get('data', []):
                print(f"- {model['id']}")
                if 'capabilities' in model:
                    caps = model['capabilities']
                    print(f"  åç«¯: {caps.get('backend', 'unknown')}")
                    print(f"  è§†è§‰: {caps.get('vision', False)}")
        else:
            if not args.image:
                print("è­¦å‘Š: æœªæä¾›å›¾åƒï¼Œå°†è¿›è¡Œçº¯æ–‡æœ¬æµ‹è¯•")
            
            print(f"\næ­£åœ¨æµ‹è¯• GGUF åç«¯...")
            print(f"æœåŠ¡å™¨: {args.url}")
            print(f"æç¤º: {args.text}")
            if args.image:
                print(f"å›¾åƒ: {args.image}")
            print(f"æµå¼: {args.stream}")
            print("-" * 60)
            
            start_time = time.time()
            response = client.chat_completion(
                text=args.text,
                image_path=args.image,
                max_tokens=args.max_tokens,
                temperature=args.temperature,
                stream=args.stream
            )
            end_time = time.time()
            print(f"\næ¶ˆè€—æ—¶é—´: {end_time - start_time:.2f} ç§’")

            if args.stream:
                # æµå¼å“åº”å·²ç»åœ¨_handle_stream_responseä¸­æ‰“å°
                pass
            else:
                content = response.get('choices', [{}])[0].get('message', {}).get('content', '')
                usage = response.get('usage', {})

                print("\nå“åº”å†…å®¹:")
                print(content)
                print("\nä½¿ç”¨ç»Ÿè®¡:")
                print(f"- æç¤ºtokens: {usage.get('prompt_tokens', 'N/A')}")
                print(f"- å®Œæˆtokens: {usage.get('completion_tokens', 'N/A')}")
                print(f"- æ€»tokens: {usage.get('total_tokens', 'N/A')}")

    except Exception as e:
        print(f"é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()

# python demo_ppocrvl_gguf_client.py --image test.png
```
