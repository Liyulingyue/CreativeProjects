#!/usr/bin/env python3
"""
å¯¼å‡º PaddleOCR-VL è¯­è¨€æ¨¡å‹éƒ¨åˆ†
æå– LLM æƒé‡å¹¶å‡†å¤‡ GGUF è½¬æ¢æ‰€éœ€çš„æ ¼å¼
"""

import argparse
import copy
import json
from pathlib import Path
import torch
from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer

def sanitize_text_config(base_config: AutoConfig) -> AutoConfig:
    """Create a text-only config compatible with Ernie4.5 causal LM."""
    text_config = copy.deepcopy(base_config)
    text_config.architectures = ["Ernie4_5ForCausalLM"]
    text_config.model_type = "ernie4_5"
    text_config.is_encoder_decoder = False
    text_config.add_cross_attention = False
    text_config.tie_encoder_decoder = False

    if getattr(text_config, "num_key_value_heads", None) is None:
        text_config.num_key_value_heads = text_config.num_attention_heads

    text_config.auto_map = {
        "AutoConfig": "configuration_paddleocr_vl.PaddleOCRVLConfig",
        "AutoModelForCausalLM": "modeling_paddleocr_vl.Ernie4_5ForCausalLM",
    }

    return text_config

def export_language_model(input_path: str, output_path: str, create_hf_checkpoint: bool = True):
    """
    å¯¼å‡ºè¯­è¨€æ¨¡å‹éƒ¨åˆ†

    Args:
        input_path: è¾“å…¥çš„å®Œæ•´æ¨¡å‹è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„
        create_hf_checkpoint: æ˜¯å¦åˆ›å»ºHuggingFaceæ ¼å¼çš„æ£€æŸ¥ç‚¹
    """
    print(f"æ­£åœ¨åŠ è½½å®Œæ•´æ¨¡å‹: {input_path}")
    full_model = AutoModelForCausalLM.from_pretrained(
        input_path,
        trust_remote_code=True,
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True
    )

    base_config = AutoConfig.from_pretrained(input_path, trust_remote_code=True)
    text_config = sanitize_text_config(base_config)

    print("æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"æ¨¡å‹æ¶æ„: {type(full_model)}")
    print(f"LLM éƒ¨åˆ†: {type(full_model.model)}")

    # æå– LLM éƒ¨åˆ†
    llm_model = full_model.model
    lm_head = full_model.lm_head

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(output_path)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜ä¸º PyTorch æ ¼å¼
    llm_weights_path = output_dir / "llm_model.pt"
    lm_head_path = output_dir / "lm_head.pt"
    config_path = output_dir / "llm_config.json"

    print(f"ä¿å­˜ LLM æƒé‡åˆ°: {llm_weights_path}")
    llm_state = llm_model.state_dict()
    torch.save(llm_state, llm_weights_path)

    print(f"ä¿å­˜ LM Head åˆ°: {lm_head_path}")
    lm_head_state = lm_head.state_dict()
    torch.save(lm_head_state, lm_head_path)

    # ä¿å­˜é…ç½®
    print(f"ä¿å­˜é…ç½®åˆ°: {config_path}")
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(text_config.to_dict(), f, indent=2, ensure_ascii=False)

    # åˆ›å»º HuggingFace æ ¼å¼æ£€æŸ¥ç‚¹ï¼ˆç”¨äº GGUF è½¬æ¢ï¼‰
    if create_hf_checkpoint:
        hf_dir = output_dir / "hf_model"
        hf_dir.mkdir(exist_ok=True)

        print(f"åˆ›å»º HuggingFace æ ¼å¼æ£€æŸ¥ç‚¹: {hf_dir}")

        # ä½¿ç”¨ from_config åˆ›å»ºæ¨¡å‹ï¼Œé¿å…é…ç½®ä¸åŒ¹é…çš„é—®é¢˜
        try:
            ernie_model = AutoModelForCausalLM.from_config(text_config, trust_remote_code=True)
        except Exception as e:
            print(f"è­¦å‘Š: from_config å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•: {e}")
            # å¤‡ç”¨æ–¹æ³•ï¼šåˆ›å»ºæ ‡å‡† Ernie æ¨¡å‹å¹¶æ‰‹åŠ¨åŠ è½½æƒé‡
            from transformers import ErnieForCausalLM
            ernie_config = copy.deepcopy(text_config)
            ernie_config.model_type = "ernie"
            ernie_config.architectures = ["ErnieForCausalLM"]
            # æ·»åŠ ç¼ºå¤±çš„é…ç½®é¡¹
            if not hasattr(ernie_config, 'type_vocab_size'):
                ernie_config.type_vocab_size = 2
            if not hasattr(ernie_config, 'max_position_embeddings'):
                ernie_config.max_position_embeddings = getattr(ernie_config, 'max_position_embeddings', 4096)
            ernie_model = ErnieForCausalLM(ernie_config)

        # åŠ è½½æˆ‘ä»¬æå–çš„æƒé‡
        ernie_model.model.load_state_dict(llm_state)
        ernie_model.lm_head.load_state_dict(lm_head_state)

        # ä¿å­˜ä¸º HuggingFace æ ¼å¼
        ernie_model.save_pretrained(hf_dir)

        # å¤åˆ¶ tokenizer æ–‡ä»¶
        tokenizer = AutoTokenizer.from_pretrained(input_path, trust_remote_code=True)
        tokenizer.save_pretrained(hf_dir)

        print(f"âœ… HuggingFace æ£€æŸ¥ç‚¹åˆ›å»ºå®Œæˆ: {hf_dir}")

    # ç»Ÿè®¡å‚æ•°
    total_params = sum(param.numel() for param in llm_model.parameters())
    print(f"\nğŸ“Š è¯­è¨€æ¨¡å‹ç»Ÿè®¡:")
    print(f"æ€»å‚æ•°: {total_params:,}")
    print(f"   - éšè—å±‚æ•°: {text_config.num_hidden_layers}")
    print(f"   - æ³¨æ„åŠ›å¤´æ•°: {text_config.num_attention_heads}")
    print(f"   - è¯æ±‡è¡¨å¤§å°: {text_config.vocab_size}")

    # æ¸…ç†å†…å­˜
    del full_model, llm_model, lm_head
    torch.cuda.empty_cache() if torch.cuda.is_available() else None

    print("âœ… è¯­è¨€æ¨¡å‹å¯¼å‡ºå®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
    print("ğŸ“‹ åŒ…å«æ–‡ä»¶:")
    for file in sorted(output_dir.glob("*")):
        if file.is_file():
            size_mb = file.stat().st_size / (1024 * 1024)
            print(f"   - {file.name}: {size_mb:.1f} MB")
    if create_hf_checkpoint:
        print("ğŸ“‹ HF æ£€æŸ¥ç‚¹æ–‡ä»¶:")
        for file in sorted(hf_dir.glob("*")):
            if file.is_file():
                size_mb = file.stat().st_size / (1024 * 1024)
                print(f"   - {file.name}: {size_mb:.1f} MB")
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å¯¼å‡º PaddleOCR-VL è¯­è¨€æ¨¡å‹éƒ¨åˆ†")
    parser.add_argument("--input-path", type=str, required=True,
                       help="è¾“å…¥çš„å®Œæ•´æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output-path", type=str, required=True,
                       help="è¾“å‡ºè·¯å¾„")
    parser.add_argument("--no-hf-checkpoint", action="store_true",
                       help="ä¸åˆ›å»º HuggingFace æ ¼å¼æ£€æŸ¥ç‚¹")

    args = parser.parse_args()

    export_language_model(
        args.input_path,
        args.output_path,
        create_hf_checkpoint=not args.no_hf_checkpoint
    )