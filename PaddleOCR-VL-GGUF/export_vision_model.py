#!/usr/bin/env python3
"""
å¯¼å‡º PaddleOCR-VL è§†è§‰æ¨¡å‹çš„å¾®ç¼©ç‰ˆ
åªåŒ…å«è§†è§‰ç¼–ç å™¨å’ŒæŠ•å½±å±‚ï¼Œå»é™¤ LLM éƒ¨åˆ†ä»¥å‡å°‘å†…å­˜å ç”¨
"""

import torch
import torch.nn as nn
from transformers import AutoProcessor, AutoModelForCausalLM
import argparse
import os
from pathlib import Path
import json

class PaddleOCRVisionOnlyModel(nn.Module):
    """
    å¾®ç¼©ç‰ˆ PaddleOCR-VL æ¨¡å‹ï¼ŒåªåŒ…å«è§†è§‰ç¼–ç å™¨å’ŒæŠ•å½±å±‚
    """
    def __init__(self, visual_encoder, projector, config):
        super().__init__()
        self.visual = visual_encoder
        self.mlp_AR = projector
        self.config = config

    def forward(self, pixel_values, image_grid_thw, position_ids, vision_return_embed_list=True,
                interpolate_pos_encoding=True, sample_indices=None, cu_seqlens=None,
                return_pooler_output=False, use_rope=True, window_size=-1):
        """
        å‰å‘ä¼ æ’­ï¼Œåªå¤„ç†è§†è§‰è¾“å…¥
        """
        return self.visual(
            pixel_values=pixel_values,
            image_grid_thw=image_grid_thw,
            position_ids=position_ids,
            vision_return_embed_list=vision_return_embed_list,
            interpolate_pos_encoding=interpolate_pos_encoding,
            sample_indices=sample_indices,
            cu_seqlens=cu_seqlens,
            return_pooler_output=return_pooler_output,
            use_rope=use_rope,
            window_size=window_size,
        )

def export_vision_model(input_path: str, output_path: str):
    """
    å¯¼å‡ºè§†è§‰æ¨¡å‹å¾®ç¼©ç‰ˆ

    Args:
        input_path: åŸå§‹å®Œæ•´æ¨¡å‹è·¯å¾„
        output_path: è¾“å‡ºè·¯å¾„
    """
    print(f"æ­£åœ¨åŠ è½½å®Œæ•´æ¨¡å‹: {input_path}")
    full_model = AutoModelForCausalLM.from_pretrained(
        input_path,
        trust_remote_code=True,
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True
    ).to("cpu")

    print("æå–è§†è§‰ç»„ä»¶...")
    visual_encoder = full_model.visual
    projector = full_model.mlp_AR
    config = full_model.config

    # åˆ›å»ºå¾®ç¼©ç‰ˆæ¨¡å‹
    vision_model = PaddleOCRVisionOnlyModel(visual_encoder, projector, config)

    # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    vision_model.eval()

    # ä¿å­˜æ¨¡å‹
    os.makedirs(output_path, exist_ok=True)

    print(f"ä¿å­˜å¾®ç¼©ç‰ˆæ¨¡å‹åˆ°: {output_path}")
    # åˆ†åˆ«ä¿å­˜è§†è§‰ç¼–ç å™¨å’ŒæŠ•å½±å±‚çš„çŠ¶æ€
    vision_state = visual_encoder.state_dict()
    projector_state = projector.state_dict()

    torch.save({
        'visual_encoder': vision_state,
        'projector': projector_state,
        'config': config.to_dict()
    }, os.path.join(output_path, 'vision_model.pt'))

    # å¤åˆ¶ processor å’Œ tokenizer æ–‡ä»¶
    processor = AutoProcessor.from_pretrained(input_path, trust_remote_code=True)
    processor.save_pretrained(output_path)

    # æŠ½å–å¹¶ä¿å­˜å¿…è¦çš„ä»£ç æ–‡ä»¶ï¼Œå®ç°æ¨¡å‹ç‹¬ç«‹åŠ è½½
    import shutil
    print("æŠ½å–æ¨¡å‹å®šä¹‰ä»£ç ...")
    code_files = ['configuration_paddleocr_vl.py', 'modeling_paddleocr_vl.py']
    for filename in code_files:
        src_file = os.path.join(input_path, filename)
        if os.path.exists(src_file):
            shutil.copy(src_file, os.path.join(output_path, filename))
            print(f"   - å·²æŠ½å–: {filename}")
        else:
            print(f"   - è­¦å‘Š: æœªæ‰¾åˆ° {filename}")

    # ä¿å­˜é…ç½®ä¿¡æ¯
    with open(os.path.join(output_path, 'model_info.json'), 'w') as f:
        json.dump({
            'model_type': 'paddleocr_vl_vision_only',
            'original_model': input_path,
            'components': ['visual_encoder', 'projector', 'processor'],
            'torch_dtype': 'float32'
        }, f, indent=2)

    # æ¸…ç†å†…å­˜
    del full_model
    torch.cuda.empty_cache() if torch.cuda.is_available() else None

    print("âœ… è§†è§‰æ¨¡å‹å¯¼å‡ºå®Œæˆï¼")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {output_path}")
    print("ğŸ“‹ åŒ…å«æ–‡ä»¶:")
    for file in os.listdir(output_path):
        print(f"   - {file}")

def load_vision_model(model_path: str, device: str = "cpu"):
    """
    åŠ è½½å¾®ç¼©ç‰ˆè§†è§‰æ¨¡å‹

    Args:
        model_path: æ¨¡å‹è·¯å¾„
        device: è®¾å¤‡ ('cpu' æˆ– 'cuda')

    Returns:
        vision_model: è§†è§‰æ¨¡å‹
        processor: processor
    """
    print("æ­£åœ¨ç›´æ¥åŠ è½½å¾®ç¼©ç‰ˆè§†è§‰æ¨¡å‹...")

    # åŠ è½½ä¿å­˜çš„çŠ¶æ€
    checkpoint = torch.load(os.path.join(model_path, 'vision_model.pt'), map_location=device, weights_only=False)

    # åŠ è½½é…ç½®
    config_dict = checkpoint['config']
    import sys
    sys.path.append('PaddlePaddle/PaddleOCR-VL')
    from configuration_paddleocr_vl import PaddleOCRVLConfig
    config = PaddleOCRVLConfig(**config_dict)

    # è·å–åŸå§‹æ¨¡å‹è·¯å¾„æ¥é‡å»ºç»“æ„
    with open(os.path.join(model_path, 'model_info.json'), 'r') as f:
        info = json.load(f)
    original_path = info['original_model']

    # å¿«é€ŸåŠ è½½å®Œæ•´æ¨¡å‹è·å–ç»“æ„
    full_model = AutoModelForCausalLM.from_pretrained(
        original_path,
        trust_remote_code=True,
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True
    ).to(device)

    # åŠ è½½ä¿å­˜çš„çŠ¶æ€åˆ°ç»„ä»¶ä¸­
    full_model.visual.load_state_dict(checkpoint['visual_encoder'])
    full_model.mlp_AR.load_state_dict(checkpoint['projector'])

    # åˆ›å»ºå¾®ç¼©ç‰ˆæ¨¡å‹
    vision_model = PaddleOCRVisionOnlyModel(
        full_model.visual,
        full_model.mlp_AR,
        config
    )

    vision_model.to(device)
    vision_model.eval()

    # æ¸…ç†ä¸´æ—¶æ¨¡å‹ï¼ˆä¿ç•™è§†è§‰ç»„ä»¶ï¼‰
    del full_model.model
    del full_model.lm_head
    del full_model
    torch.cuda.empty_cache() if torch.cuda.is_available() else None

    # åŠ è½½ processor
    processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)

    return vision_model, processor

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="å¯¼å‡º PaddleOCR-VL è§†è§‰æ¨¡å‹å¾®ç¼©ç‰ˆ")
    parser.add_argument("--input-path", type=str, required=True,
                       help="åŸå§‹å®Œæ•´æ¨¡å‹è·¯å¾„")
    parser.add_argument("--output-path", type=str, required=True,
                       help="è¾“å‡ºè·¯å¾„")

    args = parser.parse_args()

    export_vision_model(args.input_path, args.output_path)