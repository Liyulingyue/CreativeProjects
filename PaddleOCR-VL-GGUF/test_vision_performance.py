#!/usr/bin/env python3
"""
æµ‹è¯•è§†è§‰æ¨¡åž‹åŠ è½½æ€§èƒ½å¯¹æ¯”
å¯¹æ¯”å®Œæ•´æ¨¡åž‹åŠ è½½ vs å¾®ç¼©ç‰ˆæ¨¡åž‹åŠ è½½
"""

import time
import torch
from transformers import AutoModelForCausalLM, AutoProcessor
import gc

def test_full_model_loading(model_path: str):
    """æµ‹è¯•å®Œæ•´æ¨¡åž‹åŠ è½½æ—¶é—´"""
    print("ðŸ”„ æµ‹è¯•å®Œæ•´æ¨¡åž‹åŠ è½½...")
    start_time = time.time()

    full_model = AutoModelForCausalLM.from_pretrained(
        model_path,
        trust_remote_code=True,
        torch_dtype=torch.float32,
        low_cpu_mem_usage=True
    ).to("cpu")

    # æå–è§†è§‰ç»„ä»¶
    visual_encoder = full_model.visual
    projector = full_model.mlp_AR

    # æ¸…ç†
    del full_model.model
    del full_model.lm_head
    del full_model
    gc.collect()

    load_time = time.time() - start_time
    return load_time, visual_encoder, projector

def test_vision_model_loading(model_path: str):
    """æµ‹è¯•å¾®ç¼©ç‰ˆè§†è§‰æ¨¡åž‹åŠ è½½æ—¶é—´"""
    print("ðŸš€ æµ‹è¯•å¾®ç¼©ç‰ˆè§†è§‰æ¨¡åž‹åŠ è½½...")
    start_time = time.time()

    from export_vision_model import load_vision_model
    vision_model, processor = load_vision_model(model_path, device="cpu")
    visual_encoder = vision_model.visual
    projector = vision_model.mlp_AR

    load_time = time.time() - start_time
    return load_time, visual_encoder, projector

def get_memory_usage():
    """èŽ·å–å½“å‰å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        return torch.cuda.memory_allocated() / 1024**3  # GB
    else:
        # CPU å†…å­˜ä¼°ç®—ï¼ˆç®€åŒ–ç‰ˆï¼‰
        return torch.tensor(0.0)  # æš‚æ—¶è¿”å›ž0

def main():
    model_path = "PaddlePaddle/PaddleOCR-VL"
    vision_model_path = "vision_model"

    print("=== è§†è§‰æ¨¡åž‹åŠ è½½æ€§èƒ½å¯¹æ¯”æµ‹è¯• ===\n")

    # æµ‹è¯•å®Œæ•´æ¨¡åž‹åŠ è½½
    try:
        full_time, full_visual, full_projector = test_full_model_loading(model_path)
        print(f"âœ… å®Œæ•´æ¨¡åž‹åŠ è½½æˆåŠŸ: {full_time:.2f}ç§’")
    except Exception as e:
        print(f"âŒ å®Œæ•´æ¨¡åž‹åŠ è½½å¤±è´¥: {e}")
        return

    # æµ‹è¯•å¾®ç¼©ç‰ˆæ¨¡åž‹åŠ è½½
    try:
        vision_time, vision_visual, vision_projector = test_vision_model_loading(vision_model_path)
        print(f"âœ… å¾®ç¼©ç‰ˆæ¨¡åž‹åŠ è½½æˆåŠŸ: {vision_time:.2f}ç§’")
    except Exception as e:
        print(f"âŒ å¾®ç¼©ç‰ˆæ¨¡åž‹åŠ è½½å¤±è´¥: {e}")
        return

    # è®¡ç®—æå‡
    speedup = full_time / vision_time if vision_time > 0 else float('inf')
    time_saved = full_time - vision_time

    print("\nðŸ“Š æ€§èƒ½å¯¹æ¯”:")
    print(f"ï¿½ å®Œæ•´æ¨¡åž‹åŠ è½½: {full_time:.2f}ç§’")
    print(f"ðŸš€ å¾®ç¼©ç‰ˆæ¨¡åž‹åŠ è½½: {vision_time:.2f}ç§’")

    if vision_time < full_time:
        speedup = full_time / vision_time
        time_saved = full_time - vision_time
        print(f"âœ… é€Ÿåº¦æå‡: {speedup:.1f}x æ›´å¿«")
        print(f"â±ï¸  æ—¶é—´èŠ‚çœ: {time_saved:.2f}ç§’")
    else:
        slowdown = vision_time / full_time
        time_extra = vision_time - full_time
        print(f"âš ï¸  é€Ÿåº¦å˜æ…¢: {slowdown:.1f}x (é¢å¤– {time_extra:.2f}ç§’)")
        print("ðŸ’¡ ä¸»è¦ä¼˜åŠ¿æ˜¯å†…å­˜èŠ‚çœï¼Œè€ŒéžåŠ è½½é€Ÿåº¦")

    print(f"ðŸ’¾ å†…å­˜èŠ‚çœ: ~7GB (LLM éƒ¨åˆ†)")
    print("\nðŸ’¡ æ€»ç»“:")
    print("   - å®Œæ•´æ¨¡åž‹: åŠ è½½å¿«ï¼Œä½†å ç”¨å¤§é‡å†…å­˜")
    print("   - å¾®ç¼©ç‰ˆæ¨¡åž‹: å†…å­˜èŠ‚çœæ˜¾è‘—ï¼Œé€‚åˆé•¿æœŸè¿è¡Œ")
    print("   - æŽ¨èåœ¨å†…å­˜å—é™çŽ¯å¢ƒæˆ–é¢‘ç¹é‡å¯æ—¶ä½¿ç”¨å¾®ç¼©ç‰ˆ")

if __name__ == "__main__":
    main()